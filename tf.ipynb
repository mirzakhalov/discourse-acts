{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features setup correctly\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Bidirectional, Conv2D\n",
    "import numpy\n",
    "from gensim.models.fasttext import load_facebook_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from features import Features\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk import tokenize\n",
    "import tensorflow as tf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenizer = TweetTokenizer()\n",
    "global comments\n",
    "global tokens\n",
    "global max_len\n",
    "global labels\n",
    "parents = []\n",
    "\n",
    "\n",
    "trained_model = None\n",
    "f = Features()\n",
    "f.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "global fasttext_model\n",
    "\n",
    "discourse = ['other', 'agreement', 'announcement', 'appreciation', 'humor', 'answer', 'elaboration', 'negativereaction',\n",
    "             'question', 'disagreement']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(filename):\n",
    "    load_json_data = []\n",
    "    count = 0\n",
    "    with open(filename) as jsonfile:\n",
    "        for line in jsonfile:\n",
    "            jline = json.loads(line)\n",
    "            load_json_data.append(jline)\n",
    "            count += 1\n",
    "    return load_json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemma(word):\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_body(load_data):\n",
    "    global comments\n",
    "    global tokens\n",
    "    global max_len\n",
    "    global labels\n",
    "    comments = []\n",
    "    labels = []\n",
    "    count = 0\n",
    "    for jline in load_data:\n",
    "        for post in jline['posts']:\n",
    "            try:\n",
    "                features = []\n",
    "                b = post['body']\n",
    "                p = f.getParentBody(jline, post['id'])\n",
    "                label = discourse.index(post['majority_type'])\n",
    "                comments.append(b)\n",
    "                labels.append(label)\n",
    "                parents.append(p)\n",
    "            except Exception as e:\n",
    "                count += 1\n",
    "    print(count)\n",
    "    print(len(comments))\n",
    "    print(len(parents))\n",
    "    print(len(labels))\n",
    "    return \"Done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14376\n",
      "101981\n",
      "101981\n",
      "101981\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "global comments\n",
    "global labels\n",
    "json_data = get_data(\"coarse_discourse_dump_reddit.jsonlist\")\n",
    "print(process_body(json_data))\n",
    "X = comments\n",
    "Y = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101981, 1, 7495)\n"
     ]
    }
   ],
   "source": [
    "global comments\n",
    "global labels\n",
    "vectorizer = TfidfVectorizer(ngram_range = (1,3), min_df=(100/len(X))).fit(X)\n",
    "tf_X = vectorizer.transform(X).toarray()\n",
    "tf_parent = vectorizer.transform(parents).toarray()#//shape - (3,6)\n",
    "tf_X = tf_X[:, None, :]\n",
    "\n",
    "print(tf_X.shape)\n",
    "\n",
    "# train(tf_X, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(tf_X, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_model(data):\n",
    "    global trained_model\n",
    "    input_shape = data[0].shape\n",
    "    # After trying a bunch of different methods, this one worked the best\n",
    "    t_model = tf.keras.Sequential([\n",
    "        Bidirectional(LSTM(100, input_shape=input_shape)),\n",
    "        # Bidirectional(LSTM(100)),\n",
    "        # Bidirectional(LSTM(64)),\n",
    "        #hub_layer,\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.25),\n",
    "        Dense(32, activation='relu'),\n",
    "        \n",
    "        #tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    t_model.compile(optimizer='adam',\n",
    "                    loss='categorical_crossentropy',\n",
    "                    metrics=['categorical_accuracy'])\n",
    "    trained_model = t_model\n",
    "\n",
    "def train(data, labels):\n",
    "    global trained_model\n",
    "    checkpoint_path = 'training_1/cp.ckpt'\n",
    "    set_model(data)\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, save_weights_only=True, verbose=1)\n",
    "    trained_model.fit(data, to_categorical(labels), validation_split=0.1, epochs=100, batch_size=128, callbacks=[cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(filename):\n",
    "    load_json_data = []\n",
    "    count = 0\n",
    "    with open(filename) as jsonfile:\n",
    "        for line in jsonfile:\n",
    "            jline = json.loads(line)\n",
    "            load_json_data.append(jline)\n",
    "            count += 1\n",
    "    return load_json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(load_data):\n",
    "    global errors\n",
    "    global f\n",
    "    count = 0\n",
    "    count_no_author = 0\n",
    "    count_no_title = 0\n",
    "    process_data_list = []\n",
    "    process_label_list = []\n",
    "    \n",
    "    \n",
    "    \n",
    "    for jline in load_data:\n",
    "#        author = None\n",
    "#         if 'author' in jline['posts'][0]:\n",
    "#             author = jline['posts'][0]['author']\n",
    "        for post in jline['posts']:\n",
    "            try:\n",
    "                this_post = post['body']\n",
    "                \n",
    "                # Structure\n",
    "                features = f.getStructureFeatures(jline, post['id'])\n",
    "                # Content\n",
    "                b = post['body']\n",
    "                    \n",
    "                # Author\n",
    "                features.append(f.isSameAuthor(jline, post))\n",
    "                \n",
    "#                 if author == post['author']:\n",
    "#                     features.append(numpy.full(300, 1.0))\n",
    "#                 else:\n",
    "#                     features.append(numpy.full(300, 0.0))\n",
    "              \n",
    "                \n",
    "                \n",
    "#                 if 'title' in jline:\n",
    "#                     features.append(fasttext_Vec(jline['title']))\n",
    "#                 else:\n",
    "#                     features.append(numpy.zeros(300))\n",
    "                    \n",
    "                \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "#                 # Community\n",
    "#                 if 'subreddit' in jline:\n",
    "#                     features.append(fasttext_Vec(jline['subreddit']))\n",
    "#                 else:\n",
    "#                     features.append(numpy.zeros(300))\n",
    "                    \n",
    "                # Thread\n",
    "                features += f.thread_info(jline)\n",
    "                \n",
    "                \n",
    "                parent_post = f.getParentBody(jline, post['id'])\n",
    "                features.append(numpy.array(vectorizer.transform([parent_post]).toarray()[0]))\n",
    "                features.append(numpy.array(vectorizer.transform([this_post]).toarray()[0]))\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "                feature_nparr = numpy.array(features)\n",
    "                label = discourse.index(post['majority_type'])\n",
    "                process_label_list.append([label])\n",
    "                process_data_list.append(feature_nparr)\n",
    "            except Exception as e:\n",
    "                count += 1\n",
    "    print(\"Total exception count: \" + str(count))\n",
    "    print(\"No authors: \" + str(count_no_author))\n",
    "    print(\"No titles: \" + str(count_no_title))\n",
    "    process_data_list = numpy.array(process_data_list)\n",
    "    process_label_list = numpy.array(process_label_list)\n",
    "    print(process_data_list.shape)\n",
    "    print(process_label_list.shape)\n",
    "    print(\"done processing\")\n",
    "    return process_data_list, process_label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "json_data = get_data(\"coarse_discourse_dump_reddit.jsonlist\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total exception count: 14718\n",
      "No authors: 0\n",
      "No titles: 0\n",
      "(101639, 14)\n",
      "(101639, 1)\n",
      "done processing\n"
     ]
    }
   ],
   "source": [
    "data, labels = process_data(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "test_vec = vectorizer.transform([\"Hello! How are you doing?\"]).toarray()\n",
    "print(test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101639, 14)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = numpy.array([numpy.concatenate((numpy.array([datum[0][0]]), numpy.array([datum[1][0]]), numpy.array([datum[2][0]]), numpy.array([datum[3][0]]), numpy.array([datum[4][0]]), numpy.array([datum[5][0]]), numpy.array([datum[6][0]]), numpy.array([datum[7][0]]), numpy.array([datum[8][0]]), numpy.array([datum[9][0]]), numpy.array([datum[10][0]]), numpy.array([datum[11][0]]), numpy.array(datum[12]), numpy.array(datum[13])), axis=None) for datum in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(numpy.array(new_data).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2],\n",
       "       [6],\n",
       "       [6],\n",
       "       ...,\n",
       "       [5],\n",
       "       [5],\n",
       "       [6]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101639, 1, 15002)\n"
     ]
    }
   ],
   "source": [
    "new_data = new_data[:, None, :]\n",
    "print(new_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = [datam[0] for datam in new_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 91475 samples, validate on 10164 samples\n",
      "Epoch 1/100\n",
      "91008/91475 [============================>.] - ETA: 0s - loss: 1.6053 - categorical_accuracy: 0.4432\n",
      "Epoch 00001: saving model to training_1/cp.ckpt\n",
      "91475/91475 [==============================] - 11s 122us/sample - loss: 1.6052 - categorical_accuracy: 0.4433 - val_loss: 1.5363 - val_categorical_accuracy: 0.4507\n",
      "Epoch 2/100\n",
      "91136/91475 [============================>.] - ETA: 0s - loss: 1.4888 - categorical_accuracy: 0.4879\n",
      "Epoch 00002: saving model to training_1/cp.ckpt\n",
      "91475/91475 [==============================] - 9s 99us/sample - loss: 1.4883 - categorical_accuracy: 0.4881 - val_loss: 1.4266 - val_categorical_accuracy: 0.5009\n",
      "Epoch 3/100\n",
      "91392/91475 [============================>.] - ETA: 0s - loss: 1.4113 - categorical_accuracy: 0.5177\n",
      "Epoch 00003: saving model to training_1/cp.ckpt\n",
      "91475/91475 [==============================] - 9s 101us/sample - loss: 1.4113 - categorical_accuracy: 0.5177 - val_loss: 1.3550 - val_categorical_accuracy: 0.5188\n",
      "Epoch 4/100\n",
      "90880/91475 [============================>.] - ETA: 0s - loss: 1.3616 - categorical_accuracy: 0.5375\n",
      "Epoch 00004: saving model to training_1/cp.ckpt\n",
      "91475/91475 [==============================] - 9s 100us/sample - loss: 1.3606 - categorical_accuracy: 0.5377 - val_loss: 1.2869 - val_categorical_accuracy: 0.5690\n",
      "Epoch 5/100\n",
      "91392/91475 [============================>.] - ETA: 0s - loss: 1.3133 - categorical_accuracy: 0.5566\n",
      "Epoch 00005: saving model to training_1/cp.ckpt\n",
      "91475/91475 [==============================] - 9s 100us/sample - loss: 1.3131 - categorical_accuracy: 0.5566 - val_loss: 1.2910 - val_categorical_accuracy: 0.5624\n",
      "Epoch 6/100\n",
      "91264/91475 [============================>.] - ETA: 0s - loss: 1.2900 - categorical_accuracy: 0.5659\n",
      "Epoch 00006: saving model to training_1/cp.ckpt\n",
      "91475/91475 [==============================] - 9s 101us/sample - loss: 1.2903 - categorical_accuracy: 0.5658 - val_loss: 1.2560 - val_categorical_accuracy: 0.5845\n",
      "Epoch 7/100\n",
      "91392/91475 [============================>.] - ETA: 0s - loss: 1.2689 - categorical_accuracy: 0.5735\n",
      "Epoch 00007: saving model to training_1/cp.ckpt\n",
      "91475/91475 [==============================] - 9s 101us/sample - loss: 1.2689 - categorical_accuracy: 0.5735 - val_loss: 1.2383 - val_categorical_accuracy: 0.5860\n",
      "Epoch 8/100\n",
      "91392/91475 [============================>.] - ETA: 0s - loss: 1.2620 - categorical_accuracy: 0.5743\n",
      "Epoch 00008: saving model to training_1/cp.ckpt\n",
      "91475/91475 [==============================] - 10s 104us/sample - loss: 1.2619 - categorical_accuracy: 0.5742 - val_loss: 1.2400 - val_categorical_accuracy: 0.5765\n",
      "Epoch 9/100\n",
      "91008/91475 [============================>.] - ETA: 0s - loss: 1.2434 - categorical_accuracy: 0.5815\n",
      "Epoch 00009: saving model to training_1/cp.ckpt\n",
      "91475/91475 [==============================] - 9s 99us/sample - loss: 1.2434 - categorical_accuracy: 0.5816 - val_loss: 1.2043 - val_categorical_accuracy: 0.5960\n",
      "Epoch 10/100\n",
      "91136/91475 [============================>.] - ETA: 0s - loss: 1.2427 - categorical_accuracy: 0.5788\n",
      "Epoch 00010: saving model to training_1/cp.ckpt\n",
      "91475/91475 [==============================] - 9s 99us/sample - loss: 1.2425 - categorical_accuracy: 0.5790 - val_loss: 1.1973 - val_categorical_accuracy: 0.5919\n",
      "Epoch 11/100\n",
      "91264/91475 [============================>.] - ETA: 0s - loss: 1.2385 - categorical_accuracy: 0.5820\n",
      "Epoch 00011: saving model to training_1/cp.ckpt\n",
      "91475/91475 [==============================] - 9s 101us/sample - loss: 1.2384 - categorical_accuracy: 0.5820 - val_loss: 1.2035 - val_categorical_accuracy: 0.5912\n",
      "Epoch 12/100\n",
      "91136/91475 [============================>.] - ETA: 0s - loss: 1.2178 - categorical_accuracy: 0.5904\n",
      "Epoch 00012: saving model to training_1/cp.ckpt\n",
      "91475/91475 [==============================] - 10s 105us/sample - loss: 1.2176 - categorical_accuracy: 0.5905 - val_loss: 1.1950 - val_categorical_accuracy: 0.5892\n",
      "Epoch 13/100\n",
      "91264/91475 [============================>.] - ETA: 0s - loss: 1.2042 - categorical_accuracy: 0.5935\n",
      "Epoch 00013: saving model to training_1/cp.ckpt\n",
      "91475/91475 [==============================] - 9s 102us/sample - loss: 1.2043 - categorical_accuracy: 0.5934 - val_loss: 1.1807 - val_categorical_accuracy: 0.5996\n",
      "Epoch 14/100\n",
      "91008/91475 [============================>.] - ETA: 0s - loss: 1.1957 - categorical_accuracy: 0.5992\n",
      "Epoch 00014: saving model to training_1/cp.ckpt\n",
      "91475/91475 [==============================] - 9s 102us/sample - loss: 1.1958 - categorical_accuracy: 0.5992 - val_loss: 1.1843 - val_categorical_accuracy: 0.5994\n",
      "Epoch 15/100\n",
      "90880/91475 [============================>.] - ETA: 0s - loss: 1.1937 - categorical_accuracy: 0.5982\n",
      "Epoch 00015: saving model to training_1/cp.ckpt\n",
      "91475/91475 [==============================] - 9s 103us/sample - loss: 1.1936 - categorical_accuracy: 0.5982 - val_loss: 1.1784 - val_categorical_accuracy: 0.6007\n",
      "Epoch 16/100\n",
      "91392/91475 [============================>.] - ETA: 0s - loss: 1.1841 - categorical_accuracy: 0.6017\n",
      "Epoch 00016: saving model to training_1/cp.ckpt\n",
      "91475/91475 [==============================] - 10s 104us/sample - loss: 1.1841 - categorical_accuracy: 0.6016 - val_loss: 1.1749 - val_categorical_accuracy: 0.6053\n",
      "Epoch 17/100\n",
      "90880/91475 [============================>.] - ETA: 0s - loss: 1.1975 - categorical_accuracy: 0.5916\n",
      "Epoch 00017: saving model to training_1/cp.ckpt\n",
      "91475/91475 [==============================] - 10s 104us/sample - loss: 1.1976 - categorical_accuracy: 0.5916 - val_loss: 1.2116 - val_categorical_accuracy: 0.5775\n",
      "Epoch 18/100\n",
      "91008/91475 [============================>.] - ETA: 0s - loss: 1.1777 - categorical_accuracy: 0.6009\n",
      "Epoch 00018: saving model to training_1/cp.ckpt\n",
      "91475/91475 [==============================] - 10s 105us/sample - loss: 1.1777 - categorical_accuracy: 0.6009 - val_loss: 1.1708 - val_categorical_accuracy: 0.6010\n",
      "Epoch 19/100\n",
      "91008/91475 [============================>.] - ETA: 0s - loss: 1.1714 - categorical_accuracy: 0.6048\n",
      "Epoch 00019: saving model to training_1/cp.ckpt\n",
      "91475/91475 [==============================] - 10s 105us/sample - loss: 1.1709 - categorical_accuracy: 0.6050 - val_loss: 1.1871 - val_categorical_accuracy: 0.5997\n",
      "Epoch 20/100\n",
      "91264/91475 [============================>.] - ETA: 0s - loss: 1.1659 - categorical_accuracy: 0.6051\n",
      "Epoch 00020: saving model to training_1/cp.ckpt\n",
      "91475/91475 [==============================] - 10s 106us/sample - loss: 1.1658 - categorical_accuracy: 0.6051 - val_loss: 1.1828 - val_categorical_accuracy: 0.6045\n",
      "Epoch 21/100\n",
      "91008/91475 [============================>.] - ETA: 0s - loss: 1.1573 - categorical_accuracy: 0.6088\n",
      "Epoch 00021: saving model to training_1/cp.ckpt\n",
      "91475/91475 [==============================] - 10s 106us/sample - loss: 1.1577 - categorical_accuracy: 0.6089 - val_loss: 1.1699 - val_categorical_accuracy: 0.6068\n",
      "Epoch 22/100\n",
      "91264/91475 [============================>.] - ETA: 0s - loss: 1.1552 - categorical_accuracy: 0.6108\n",
      "Epoch 00022: saving model to training_1/cp.ckpt\n",
      "91475/91475 [==============================] - 9s 104us/sample - loss: 1.1551 - categorical_accuracy: 0.6109 - val_loss: 1.1579 - val_categorical_accuracy: 0.6112\n",
      "Epoch 23/100\n",
      "91008/91475 [============================>.] - ETA: 0s - loss: 1.1601 - categorical_accuracy: 0.6070\n",
      "Epoch 00023: saving model to training_1/cp.ckpt\n",
      "91475/91475 [==============================] - 9s 102us/sample - loss: 1.1606 - categorical_accuracy: 0.6069 - val_loss: 1.1832 - val_categorical_accuracy: 0.6013\n",
      "Epoch 24/100\n",
      "91008/91475 [============================>.] - ETA: 0s - loss: 1.1485 - categorical_accuracy: 0.6115\n",
      "Epoch 00024: saving model to training_1/cp.ckpt\n",
      "91475/91475 [==============================] - 9s 103us/sample - loss: 1.1485 - categorical_accuracy: 0.6117 - val_loss: 1.1867 - val_categorical_accuracy: 0.5966\n",
      "Epoch 25/100\n",
      "91008/91475 [============================>.] - ETA: 0s - loss: 1.1430 - categorical_accuracy: 0.6137\n",
      "Epoch 00025: saving model to training_1/cp.ckpt\n",
      "91475/91475 [==============================] - 9s 103us/sample - loss: 1.1426 - categorical_accuracy: 0.6138 - val_loss: 1.1827 - val_categorical_accuracy: 0.5962\n",
      "Epoch 26/100\n",
      "91264/91475 [============================>.] - ETA: 0s - loss: 1.1406 - categorical_accuracy: 0.6155\n",
      "Epoch 00026: saving model to training_1/cp.ckpt\n",
      "91475/91475 [==============================] - 9s 99us/sample - loss: 1.1408 - categorical_accuracy: 0.6154 - val_loss: 1.1639 - val_categorical_accuracy: 0.6130\n",
      "Epoch 27/100\n",
      "91136/91475 [============================>.] - ETA: 0s - loss: 1.1316 - categorical_accuracy: 0.6209\n",
      "Epoch 00027: saving model to training_1/cp.ckpt\n",
      "91475/91475 [==============================] - 9s 99us/sample - loss: 1.1314 - categorical_accuracy: 0.6208 - val_loss: 1.1473 - val_categorical_accuracy: 0.6153\n",
      "Epoch 28/100\n",
      "91136/91475 [============================>.] - ETA: 0s - loss: 1.1339 - categorical_accuracy: 0.6156\n",
      "Epoch 00028: saving model to training_1/cp.ckpt\n",
      "91475/91475 [==============================] - 9s 100us/sample - loss: 1.1339 - categorical_accuracy: 0.6157 - val_loss: 1.1661 - val_categorical_accuracy: 0.6136\n",
      "Epoch 29/100\n",
      "91392/91475 [============================>.] - ETA: 0s - loss: 1.1284 - categorical_accuracy: 0.6186\n",
      "Epoch 00029: saving model to training_1/cp.ckpt\n",
      "91475/91475 [==============================] - 9s 100us/sample - loss: 1.1287 - categorical_accuracy: 0.6185 - val_loss: 1.1734 - val_categorical_accuracy: 0.6034\n",
      "Epoch 30/100\n",
      "90880/91475 [============================>.] - ETA: 0s - loss: 1.1265 - categorical_accuracy: 0.6204\n",
      "Epoch 00030: saving model to training_1/cp.ckpt\n",
      "91475/91475 [==============================] - 9s 100us/sample - loss: 1.1267 - categorical_accuracy: 0.6203 - val_loss: 1.1547 - val_categorical_accuracy: 0.6127\n",
      "Epoch 31/100\n",
      "91008/91475 [============================>.] - ETA: 0s - loss: 1.1222 - categorical_accuracy: 0.6207\n",
      "Epoch 00031: saving model to training_1/cp.ckpt\n",
      "91475/91475 [==============================] - 9s 101us/sample - loss: 1.1219 - categorical_accuracy: 0.6209 - val_loss: 1.1863 - val_categorical_accuracy: 0.5985\n",
      "Epoch 32/100\n",
      "91392/91475 [============================>.] - ETA: 0s - loss: 1.1218 - categorical_accuracy: 0.6206\n",
      "Epoch 00032: saving model to training_1/cp.ckpt\n",
      "91475/91475 [==============================] - 9s 100us/sample - loss: 1.1218 - categorical_accuracy: 0.6206 - val_loss: 1.1661 - val_categorical_accuracy: 0.6106\n",
      "Epoch 33/100\n",
      "91008/91475 [============================>.] - ETA: 0s - loss: 1.1137 - categorical_accuracy: 0.6233\n",
      "Epoch 00033: saving model to training_1/cp.ckpt\n",
      "91475/91475 [==============================] - 9s 101us/sample - loss: 1.1131 - categorical_accuracy: 0.6235 - val_loss: 1.1455 - val_categorical_accuracy: 0.6155\n",
      "Epoch 34/100\n",
      "90880/91475 [============================>.] - ETA: 0s - loss: 1.1080 - categorical_accuracy: 0.6265\n",
      "Epoch 00034: saving model to training_1/cp.ckpt\n",
      "91475/91475 [==============================] - 9s 100us/sample - loss: 1.1082 - categorical_accuracy: 0.6265 - val_loss: 1.1615 - val_categorical_accuracy: 0.6092\n",
      "Epoch 35/100\n",
      "91264/91475 [============================>.] - ETA: 0s - loss: 1.1104 - categorical_accuracy: 0.6252\n",
      "Epoch 00035: saving model to training_1/cp.ckpt\n",
      "91475/91475 [==============================] - 9s 100us/sample - loss: 1.1103 - categorical_accuracy: 0.6253 - val_loss: 1.1811 - val_categorical_accuracy: 0.6002\n",
      "Epoch 36/100\n",
      "91136/91475 [============================>.] - ETA: 0s - loss: 1.1057 - categorical_accuracy: 0.6283\n",
      "Epoch 00036: saving model to training_1/cp.ckpt\n",
      "91475/91475 [==============================] - 9s 100us/sample - loss: 1.1060 - categorical_accuracy: 0.6283 - val_loss: 1.1519 - val_categorical_accuracy: 0.6127\n",
      "Epoch 37/100\n",
      "91392/91475 [============================>.] - ETA: 0s - loss: 1.0999 - categorical_accuracy: 0.6295\n",
      "Epoch 00037: saving model to training_1/cp.ckpt\n",
      "91475/91475 [==============================] - 9s 102us/sample - loss: 1.1000 - categorical_accuracy: 0.6295 - val_loss: 1.1794 - val_categorical_accuracy: 0.5982\n",
      "Epoch 38/100\n",
      "91136/91475 [============================>.] - ETA: 0s - loss: 1.0985 - categorical_accuracy: 0.6313\n",
      "Epoch 00038: saving model to training_1/cp.ckpt\n",
      "91475/91475 [==============================] - 9s 104us/sample - loss: 1.0985 - categorical_accuracy: 0.6314 - val_loss: 1.1650 - val_categorical_accuracy: 0.6056\n",
      "Epoch 39/100\n",
      "91392/91475 [============================>.] - ETA: 0s - loss: 1.0924 - categorical_accuracy: 0.6310\n",
      "Epoch 00039: saving model to training_1/cp.ckpt\n",
      "91475/91475 [==============================] - 10s 104us/sample - loss: 1.0923 - categorical_accuracy: 0.6310 - val_loss: 1.1536 - val_categorical_accuracy: 0.6110\n",
      "Epoch 40/100\n",
      "91392/91475 [============================>.] - ETA: 0s - loss: 1.0881 - categorical_accuracy: 0.6332\n",
      "Epoch 00040: saving model to training_1/cp.ckpt\n",
      "91475/91475 [==============================] - 10s 105us/sample - loss: 1.0881 - categorical_accuracy: 0.6333 - val_loss: 1.2061 - val_categorical_accuracy: 0.5839\n",
      "Epoch 41/100\n",
      "91008/91475 [============================>.] - ETA: 0s - loss: 1.0938 - categorical_accuracy: 0.6297\n",
      "Epoch 00041: saving model to training_1/cp.ckpt\n",
      "91475/91475 [==============================] - 9s 102us/sample - loss: 1.0942 - categorical_accuracy: 0.6295 - val_loss: 1.1676 - val_categorical_accuracy: 0.6103\n",
      "Epoch 42/100\n",
      "91264/91475 [============================>.] - ETA: 0s - loss: 1.0904 - categorical_accuracy: 0.6318\n",
      "Epoch 00042: saving model to training_1/cp.ckpt\n",
      "91475/91475 [==============================] - 10s 106us/sample - loss: 1.0903 - categorical_accuracy: 0.6320 - val_loss: 1.1719 - val_categorical_accuracy: 0.6066\n",
      "Epoch 43/100\n",
      "39552/91475 [===========>..................] - ETA: 5s - loss: 1.0872 - categorical_accuracy: 0.6326"
     ]
    }
   ],
   "source": [
    "train(new_data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101639, 14)\n",
      "[array([0.]) array([2553.]) array([501.]) array([29.]) array([0.])\n",
      " array([0.]) array([0.]) array([0.]) array([4.]) array([3.]) array([1.])\n",
      " array([1.])\n",
      " array([0.04366303, 0.        , 0.        , ..., 0.        , 0.        ,\n",
      "       0.        ])\n",
      " array([0.04366303, 0.        , 0.        , ..., 0.        , 0.        ,\n",
      "       0.        ])]\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.externals import joblib\n",
    "#\n",
    "from sklearn.datasets import load_iris\n",
    "#\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def run_KNN(data):\n",
    "    X_train = data[\"x_tr\"]\n",
    "    X_test = data[\"x_te\"]\n",
    "    y_train = data[\"y_tr\"]\n",
    "    y_test = data[\"y_te\"]\n",
    "\n",
    "    # training the model on training set\n",
    "    knn = KNeighborsClassifier(n_neighbors=3)\n",
    "    knn.fit(X_train, y_train)\n",
    "\n",
    "    # making predictions on the testing set\n",
    "    y_pred_train = knn.predict(X_train)\n",
    "    y_pred_test = knn.predict(X_test)\n",
    "\n",
    "    # comparing actual response values (y_test) with predicted response values (y_pred)\n",
    "    print(\"KNN Training Accuracy:\", metrics.accuracy_score(y_train, y_pred_train))\n",
    "    print(\"KNN Testing Accuracy:\", metrics.accuracy_score(y_test, y_pred_test))\n",
    "\n",
    "    # Example of making prediction for out of sample data\n",
    "    # sample = [[3, 5, 4, 2], [2, 3, 5, 4]] # make sure it is proper size\n",
    "    # preds = knn.predict(sample)\n",
    "    # print(\"Predictions:\", preds)\n",
    "\n",
    "    # saving the model\n",
    "    joblib.dump(knn, 'knn_model.pkl')\n",
    "\n",
    "    # To load model use: knn = joblib.load('knn_model.pkl')\n",
    "\n",
    "\n",
    "def run_SVM(data):\n",
    "    X_train = data[\"x_tr\"]\n",
    "    X_test = data[\"x_te\"]\n",
    "    y_train = data[\"y_tr\"]\n",
    "    y_test = data[\"y_te\"]\n",
    "\n",
    "    # training the model on training set\n",
    "    svm = SVC(gamma='auto', verbose=True, cache_size=101639)\n",
    "    svm.fit(X_train, y_train)\n",
    "\n",
    "    # making predictions on the testing set\n",
    "    y_pred_train = svm.predict(X_train)\n",
    "    y_pred_test = svm.predict(X_test)\n",
    "\n",
    "    # comparing actual response values (y_test) with predicted response values (y_pred)\n",
    "    print(\"SVM Training Accuracy:\", metrics.accuracy_score(y_train, y_pred_train))\n",
    "    print(\"SVM Testing Accuracy:\", metrics.accuracy_score(y_test, y_pred_test))\n",
    "\n",
    "    # Example of making prediction for out of sample data\n",
    "    # sample = [[3, 5, 4, 2], [2, 3, 5, 4]] # make sure it is proper size\n",
    "    # preds = svm.predict(sample)\n",
    "    # print(\"Predictions:\", preds)\n",
    "\n",
    "    # saving the model\n",
    "    joblib.dump(svm, 'svm_model.pkl')\n",
    "\n",
    "    # To load model use: knn = joblib.load('svm_model.pkl')\n",
    "\n",
    "\n",
    "def run_RandomForest(data):\n",
    "    X_train = data[\"x_tr\"]\n",
    "    X_test = data[\"x_te\"]\n",
    "    y_train = data[\"y_tr\"]\n",
    "    y_test = data[\"y_te\"]\n",
    "\n",
    "    # training the model on training set\n",
    "    rf = RandomForestClassifier(n_estimators=100, max_depth=2, random_state=0)\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    # making predictions on the testing set\n",
    "    y_pred_train = rf.predict(X_train)\n",
    "    y_pred_test = rf.predict(X_test)\n",
    "\n",
    "    # comparing actual response values (y_test) with predicted response values (y_pred)\n",
    "    print(\"RandomForest Training Accuracy:\", metrics.accuracy_score(y_train, y_pred_train))\n",
    "    print(\"RandomForest Testing Accuracy:\", metrics.accuracy_score(y_test, y_pred_test))\n",
    "\n",
    "    # Example of making prediction for out of sample data\n",
    "    # sample = [[3, 5, 4, 2], [2, 3, 5, 4]] # make sure it is proper size\n",
    "    # preds = rf.predict(sample)\n",
    "    # print(\"Predictions:\", preds)\n",
    "\n",
    "    # saving the model\n",
    "    joblib.dump(rf, 'rf_model.pkl')\n",
    "\n",
    "    # To load model use: knn = joblib.load('rf_model.pkl')\n",
    "\n",
    "\n",
    "def run_MLP(data):\n",
    "    X_train = data[\"x_tr\"]\n",
    "    X_test = data[\"x_te\"]\n",
    "    y_train = data[\"y_tr\"]\n",
    "    y_test = data[\"y_te\"]\n",
    "\n",
    "    # training the model on training set\n",
    "    mlp = MLPClassifier(solver='adam', hidden_layer_sizes=(128,))\n",
    "    mlp.fit(X_train, y_train)\n",
    "\n",
    "    # making predictions on the testing set\n",
    "    y_pred_train = mlp.predict(X_train)\n",
    "    y_pred_test = mlp.predict(X_test)\n",
    "\n",
    "    # comparing actual response values (y_test) with predicted response values (y_pred)\n",
    "    print(\"MLP Training Accuracy:\", metrics.accuracy_score(y_train, y_pred_train))\n",
    "    print(\"MLP Testing Accuracy:\", metrics.accuracy_score(y_test, y_pred_test))\n",
    "\n",
    "    # Example of making prediction for out of sample data\n",
    "    # sample = [[3, 5, 4, 2], [2, 3, 5, 4]] # make sure it is proper size\n",
    "    # preds = mlp.predict(sample)\n",
    "    # print(\"Predictions:\", preds)\n",
    "\n",
    "    # saving the model\n",
    "    joblib.dump(mlp, 'mlp_model.pkl')\n",
    "\n",
    "    # To load model use: knn = joblib.load('mlp_model.pkl')\n",
    "\n",
    "\n",
    "def run_LRG(data):\n",
    "    X_train = data[\"x_tr\"]\n",
    "    X_test = data[\"x_te\"]\n",
    "    y_train = data[\"y_tr\"]\n",
    "    y_test = data[\"y_te\"]\n",
    "\n",
    "    # training the model on training set\n",
    "    mlp = LogisticRegression()\n",
    "    mlp.fit(X_train, y_train)\n",
    "\n",
    "    # making predictions on the testing set\n",
    "    y_pred_train = mlp.predict(X_train)\n",
    "    y_pred_test = mlp.predict(X_test)\n",
    "\n",
    "    # comparing actual response values (y_test) with predicted response values (y_pred)\n",
    "    print(\"MLP Training Accuracy:\", metrics.accuracy_score(y_train, y_pred_train))\n",
    "    print(\"MLP Testing Accuracy:\", metrics.accuracy_score(y_test, y_pred_test))\n",
    "\n",
    "    # Example of making prediction for out of sample data\n",
    "    # sample = [[3, 5, 4, 2], [2, 3, 5, 4]] # make sure it is proper size\n",
    "    # preds = mlp.predict(sample)\n",
    "    # print(\"Predictions:\", preds)\n",
    "\n",
    "    # saving the model\n",
    "    joblib.dump(mlp, 'mlp_model.pkl')\n",
    "\n",
    "    # To load model use: knn = joblib.load('mlp_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(new_data, labels, test_size=0.2, random_state=1)\n",
    "data_dict = {\"x_tr\": X_train, \"x_te\": X_test, \"y_tr\": y_train, \"y_te\": y_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##############################\n",
      "\n",
      "Linear Regression:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Training Accuracy: 0.6346865737723063\n",
      "MLP Testing Accuracy: 0.5831365604092876\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print(\"#\"*30)\n",
    "print(\"\\nLinear Regression:\\n\")\n",
    "\n",
    "run_LRG(data_dict)\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##############################\n",
      "\n",
      "MULTILAYER PERCEPTRON:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:921: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print(\"#\"*30)\n",
    "print(\"\\nMULTILAYER PERCEPTRON:\\n\")\n",
    "\n",
    "run_MLP(data_dict)\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
