{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Bidirectional, Conv2D\n",
    "import numpy\n",
    "from gensim.models.fasttext import load_facebook_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from features import Features\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk import tokenize\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "\n",
    "trained_model = None\n",
    "f = Features()\n",
    "f.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "global fasttext_model\n",
    "\n",
    "discourse = ['other', 'agreement', 'announcement', 'appreciation', 'humor', 'answer', 'elaboration', 'negativereaction',\n",
    "             'question', 'disagreement']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fasttext():\n",
    "    global fasttext_model\n",
    "    fasttext_model = load_facebook_model('crawl-300d-2M-subword.bin')\n",
    "\n",
    "def get_data(filename):\n",
    "    load_json_data = []\n",
    "    count = 0\n",
    "    with open(filename) as jsonfile:\n",
    "        for line in jsonfile:\n",
    "            jline = json.loads(line)\n",
    "            load_json_data.append(jline)\n",
    "            count += 1\n",
    "    return load_json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(load_data):\n",
    "    global errors\n",
    "    global f\n",
    "    count = 0\n",
    "    count_no_author = 0\n",
    "    count_no_title = 0\n",
    "    process_data_list = []\n",
    "    process_label_list = []\n",
    "    for jline in load_data:\n",
    "        author = None\n",
    "        if 'author' in jline['posts'][0]:\n",
    "            author = jline['posts'][0]['author']\n",
    "        for post in jline['posts']:\n",
    "            try:\n",
    "                # Structure\n",
    "                features = f.getStructureFeatures(jline, post['id'])\n",
    "                # Content\n",
    "                if 'body' in post:\n",
    "                    features.append(fasttext_Vec(post['body']))\n",
    "                else:\n",
    "                    features.append(numpy.zeros(300))\n",
    "                    count_no_title += 1\n",
    "                    \n",
    "                # ge66t the vector for the parent body\n",
    "                features.append(fasttext_Vec(f.getParentBody(jline, post['id'])))\n",
    "                # Author\n",
    "                features.append(f.isSameAuthor(jline, post))\n",
    "                \n",
    "                if 'author' in post:\n",
    "                    features.append(fasttext_Vec(post['author']))\n",
    "                    \n",
    "                    if author == post['author']:\n",
    "                        features.append(numpy.full(300, 1.0))\n",
    "                    else:\n",
    "                        features.append(numpy.full(300, 0.0))\n",
    "                else:\n",
    "                    features.append(numpy.zeros(300))\n",
    "                    features.append(numpy.zeros(300))\n",
    "                    count_no_author += 1\n",
    "                \n",
    "                \n",
    "                \n",
    "                if 'title' in jline:\n",
    "                    features.append(fasttext_Vec(jline['title']))\n",
    "                else:\n",
    "                    features.append(numpy.zeros(300))\n",
    "                    \n",
    "                \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                # Community\n",
    "                if 'subreddit' in jline:\n",
    "                    features.append(fasttext_Vec(jline['subreddit']))\n",
    "                else:\n",
    "                    features.append(numpy.zeros(300))\n",
    "                    \n",
    "                # Thread\n",
    "                features += f.thread_info(jline)\n",
    "                feature_nparr = numpy.array(features)\n",
    "                label = discourse.index(post['majority_type'])\n",
    "                process_label_list.append([label])\n",
    "                process_data_list.append(feature_nparr)\n",
    "            except Exception as e:\n",
    "                count += 1\n",
    "    print(\"Total exception count: \" + str(count))\n",
    "    print(\"No authors: \" + str(count_no_author))\n",
    "    print(\"No titles: \" + str(count_no_title))\n",
    "    process_data_list = numpy.array(process_data_list)\n",
    "    process_label_list = numpy.array(process_label_list)\n",
    "    print(process_data_list.shape)\n",
    "    print(process_label_list.shape)\n",
    "    print(\"done processing\")\n",
    "    return process_data_list, process_label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def set_model(data):\n",
    "    global trained_model\n",
    "    input_shape = data[0].shape\n",
    "    # After trying a bunch of different methods, this one worked the best\n",
    "    t_model = tf.keras.Sequential([\n",
    "        \n",
    "        Bidirectional(LSTM(100, input_shape=input_shape)),\n",
    "        #Bidirectional(LSTM(100)),\n",
    "        #Bidirectional(LSTM(64)),\n",
    "        #hub_layer,\n",
    "        Dropout(0.5),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.25),\n",
    "        Dense(32, activation='relu'),\n",
    "        \n",
    "        #tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    t_model.compile(optimizer='rmsprop',\n",
    "                    loss='categorical_crossentropy',\n",
    "                    metrics=['categorical_accuracy'])\n",
    "    trained_model = t_model\n",
    "\n",
    "def train(data, labels):\n",
    "    global trained_model\n",
    "    checkpoint_path = 'training_1/cp.ckpt'\n",
    "    set_model(data)\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, save_weights_only=True, verbose=1)\n",
    "    trained_model.fit(data, to_categorical(labels), validation_split=0.1, epochs=10, batch_size=128, callbacks=[cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fasttext_Vec(body):\n",
    "    global fasttext_model\n",
    "    global lemmatizer\n",
    "    global tokenizer\n",
    "    tokens = tokenizer.tokenize(body)\n",
    "    output = numpy.zeros(300)\n",
    "    for token in tokens:\n",
    "        try:\n",
    "            output = numpy.add(output, fasttext_model[lemmatizer.lemmatize(token)])\n",
    "        except KeyError:\n",
    "            output = numpy.add(output, numpy.zeros(300))\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global data\n",
    "global labels\n",
    "\n",
    "load_fasttext()\n",
    "json_data = get_data(\"coarse_discourse_dump_reddit.jsonlist\")\n",
    "data, labels = process_data(json_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer='word', ngram_range=(2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global body\n",
    "def process_body(load_data):\n",
    "    global vectorizer\n",
    "    global f\n",
    "    global body\n",
    "    global labels\n",
    "    count = 0\n",
    "    body_t = []\n",
    "    labels_t = []\n",
    "    for jline in load_data:\n",
    "        for post in jline['posts']:\n",
    "            try:\n",
    "                features = []\n",
    "                b = post['body']\n",
    "                p = f.getParentBody(jline, post['id'])\n",
    "                label = discourse.index(post['majority_type'])\n",
    "                features.append(b)\n",
    "                features.append(p)\n",
    "                body_t.append([b])\n",
    "                labels_t.append(label)\n",
    "            except Exception as e:\n",
    "                count += 1\n",
    "    print(count)\n",
    "    print(len(body))\n",
    "    body = numpy.array(body_t)\n",
    "    labels = numpy.array(labels_t)\n",
    "    return \"Done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = \"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\"\n",
    "global hub_layer\n",
    "hub_layer = hub.KerasLayer(embedding, input_shape=[], trainable=True, dtype=tf.string)\n",
    "print(body.shape)\n",
    "train(body, labels, hub_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_model(data):\n",
    "    global trained_model\n",
    "    input_shape = data[0].shape\n",
    "    # After trying a bunch of different methods, this one worked the best\n",
    "    t_model = tf.keras.Sequential([\n",
    "        \n",
    "        Bidirectional(LSTM(100, input_shape=(len(data), 2))),\n",
    "        #Bidirectional(LSTM(100)),\n",
    "        #Bidirectional(LSTM(64)),\n",
    "        #hub_layer,\n",
    "        Dropout(0.5),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.25),\n",
    "        Dense(32, activation='relu'),\n",
    "        \n",
    "        #tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    t_model.compile(optimizer='rmsprop',\n",
    "                    loss='categorical_crossentropy',\n",
    "                    metrics=['categorical_accuracy'])\n",
    "    trained_model = t_model\n",
    "\n",
    "def train(data, labels):\n",
    "    global trained_model\n",
    "    checkpoint_path = 'training_1/cp.ckpt'\n",
    "    set_model(data)\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, save_weights_only=True, verbose=1)\n",
    "    trained_model.fit(data, to_categorical(labels), validation_split=0.1, epochs=10, batch_size=128, callbacks=[cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = get_data(\"coarse_discourse_dump_reddit.jsonlist\")\n",
    "process_body(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.externals import joblib\n",
    "#\n",
    "from sklearn.datasets import load_iris \n",
    "#\n",
    "from sklearn.model_selection import train_test_split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_KNN(data):\n",
    "    X_train = data[\"x_tr\"]\n",
    "    X_test = data[\"x_te\"]\n",
    "    y_train = data[\"y_tr\"]\n",
    "    y_test = data[\"y_te\"]\n",
    "\n",
    "    # training the model on training set \n",
    "    knn = KNeighborsClassifier(n_neighbors=3) \n",
    "    knn.fit(X_train, y_train) \n",
    "    \n",
    "    # making predictions on the testing set \n",
    "    y_pred_train = knn.predict(X_train)\n",
    "    y_pred_test = knn.predict(X_test) \n",
    "    \n",
    "    # comparing actual response values (y_test) with predicted response values (y_pred)\n",
    "    print(\"KNN Training Accuracy:\", metrics.accuracy_score(y_train, y_pred_train)) \n",
    "    print(\"KNN Testing Accuracy:\", metrics.accuracy_score(y_test, y_pred_test)) \n",
    "    \n",
    "    # Example of making prediction for out of sample data \n",
    "    # sample = [[3, 5, 4, 2], [2, 3, 5, 4]] # make sure it is proper size\n",
    "    # preds = knn.predict(sample)\n",
    "    # print(\"Predictions:\", preds) \n",
    "    \n",
    "    # saving the model \n",
    "    joblib.dump(knn, 'knn_model.pkl')\n",
    "\n",
    "    # To load model use: knn = joblib.load('knn_model.pkl')\n",
    "\n",
    "def run_SVM(data):\n",
    "    X_train = data[\"x_tr\"]\n",
    "    X_test = data[\"x_te\"]\n",
    "    y_train = data[\"y_tr\"]\n",
    "    y_test = data[\"y_te\"]\n",
    "\n",
    "    # training the model on training set \n",
    "    svm = SVC(gamma='auto')\n",
    "    svm.fit(X_train, y_train) \n",
    "    \n",
    "    # making predictions on the testing set \n",
    "    y_pred_train = svm.predict(X_train)\n",
    "    y_pred_test = svm.predict(X_test) \n",
    "    \n",
    "    # comparing actual response values (y_test) with predicted response values (y_pred)\n",
    "    print(\"SVM Training Accuracy:\", metrics.accuracy_score(y_train, y_pred_train)) \n",
    "    print(\"SVM Testing Accuracy:\", metrics.accuracy_score(y_test, y_pred_test)) \n",
    "    \n",
    "    # Example of making prediction for out of sample data \n",
    "    # sample = [[3, 5, 4, 2], [2, 3, 5, 4]] # make sure it is proper size\n",
    "    # preds = svm.predict(sample)\n",
    "    # print(\"Predictions:\", preds) \n",
    "    \n",
    "    # saving the model \n",
    "    joblib.dump(svm, 'svm_model.pkl')\n",
    "\n",
    "    # To load model use: knn = joblib.load('svm_model.pkl')\n",
    "\n",
    "def run_RandomForest(data):\n",
    "    X_train = data[\"x_tr\"]\n",
    "    X_test = data[\"x_te\"]\n",
    "    y_train = data[\"y_tr\"]\n",
    "    y_test = data[\"y_te\"]\n",
    "\n",
    "    # training the model on training set \n",
    "    rf = RandomForestClassifier(n_estimators=100, max_depth=2, random_state=0)\n",
    "    rf.fit(X_train, y_train) \n",
    "    \n",
    "    # making predictions on the testing set \n",
    "    y_pred_train = rf.predict(X_train)\n",
    "    y_pred_test = rf.predict(X_test) \n",
    "    \n",
    "    # comparing actual response values (y_test) with predicted response values (y_pred)\n",
    "    print(\"RandomForest Training Accuracy:\", metrics.accuracy_score(y_train, y_pred_train)) \n",
    "    print(\"RandomForest Testing Accuracy:\", metrics.accuracy_score(y_test, y_pred_test)) \n",
    "    \n",
    "    # Example of making prediction for out of sample data \n",
    "    # sample = [[3, 5, 4, 2], [2, 3, 5, 4]] # make sure it is proper size\n",
    "    # preds = rf.predict(sample)\n",
    "    # print(\"Predictions:\", preds) \n",
    "    \n",
    "    # saving the model \n",
    "    joblib.dump(rf, 'rf_model.pkl')\n",
    "\n",
    "    # To load model use: knn = joblib.load('rf_model.pkl')\n",
    "\n",
    "def run_MLP(data):\n",
    "    X_train = data[\"x_tr\"]\n",
    "    X_test = data[\"x_te\"]\n",
    "    y_train = data[\"y_tr\"]\n",
    "    y_test = data[\"y_te\"]\n",
    "\n",
    "    # training the model on training set \n",
    "    mlp = MLPClassifier(solver='adam', hidden_layer_sizes=(128,))\n",
    "    mlp.fit(X_train, y_train) \n",
    "    \n",
    "    # making predictions on the testing set \n",
    "    y_pred_train = mlp.predict(X_train)\n",
    "    y_pred_test = mlp.predict(X_test) \n",
    "    \n",
    "    # comparing actual response values (y_test) with predicted response values (y_pred)\n",
    "    print(\"MLP Training Accuracy:\", metrics.accuracy_score(y_train, y_pred_train)) \n",
    "    print(\"MLP Testing Accuracy:\", metrics.accuracy_score(y_test, y_pred_test)) \n",
    "    \n",
    "    # Example of making prediction for out of sample data \n",
    "    # sample = [[3, 5, 4, 2], [2, 3, 5, 4]] # make sure it is proper size\n",
    "    # preds = mlp.predict(sample)\n",
    "    # print(\"Predictions:\", preds) \n",
    "    \n",
    "    # saving the model \n",
    "    joblib.dump(mlp, 'mlp_model.pkl')\n",
    "\n",
    "    # To load model use: knn = joblib.load('mlp_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Iris testing data\n",
    "print(\"#\"*90)\n",
    "print(\"#\"*90)\n",
    "print(\"\\nIRIS DATA INITIAL TESTS *IGNORE*\\n\")\n",
    "print(\"#\"*90)\n",
    "print(\"#\"*90)\n",
    "print()\n",
    "\n",
    "\n",
    "data = data_dict\n",
    "\n",
    "print(\"\\nK NEAREST NEIGHBOR:\\n\")\n",
    "\n",
    "run_KNN(data)\n",
    "\n",
    "print()\n",
    "print(\"#\"*30)\n",
    "print(\"\\nSUPPORT VECTOR MACHINE:\\n\")\n",
    "\n",
    "run_SVM(data)\n",
    "\n",
    "print()\n",
    "print(\"#\"*30)\n",
    "print(\"\\nRANDOM FOREST:\\n\")\n",
    "\n",
    "run_RandomForest(data)\n",
    "\n",
    "print()\n",
    "print(\"#\"*30)\n",
    "print(\"\\nMULTILAYER PERCEPTRON:\\n\")\n",
    "\n",
    "run_MLP(data)\n",
    "\n",
    "print()\n",
    "\n",
    "# ***Important Note*** the saved model data will overwrite everytime you run the same model functions\n",
    "# TODO Potential More Complex Model RNN/LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.000e+00, 0.000e+00, 0.000e+00, ..., 0.000e+00, 0.000e+00,\n",
       "        0.000e+00],\n",
       "       [2.553e+03, 2.553e+03, 2.553e+03, ..., 2.553e+03, 2.553e+03,\n",
       "        2.553e+03],\n",
       "       [5.010e+02, 5.010e+02, 5.010e+02, ..., 5.010e+02, 5.010e+02,\n",
       "        5.010e+02],\n",
       "       ...,\n",
       "       [3.000e+00, 3.000e+00, 3.000e+00, ..., 3.000e+00, 3.000e+00,\n",
       "        3.000e+00],\n",
       "       [1.000e+00, 1.000e+00, 1.000e+00, ..., 1.000e+00, 1.000e+00,\n",
       "        1.000e+00],\n",
       "       [1.000e+00, 1.000e+00, 1.000e+00, ..., 1.000e+00, 1.000e+00,\n",
       "        1.000e+00]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553.\n",
      " 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553.\n",
      " 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553.\n",
      " 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553.\n",
      " 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553.\n",
      " 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553.\n",
      " 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553.\n",
      " 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553.\n",
      " 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553.\n",
      " 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553.\n",
      " 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553.\n",
      " 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553.\n",
      " 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553.\n",
      " 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553.\n",
      " 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553.\n",
      " 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553.\n",
      " 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553.\n",
      " 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553.\n",
      " 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553.\n",
      " 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553.\n",
      " 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553.\n",
      " 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553.\n",
      " 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553.\n",
      " 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553.\n",
      " 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553. 2553.]\n",
      "[501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501.\n",
      " 501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501.\n",
      " 501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501.\n",
      " 501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501.\n",
      " 501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501.\n",
      " 501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501.\n",
      " 501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501.\n",
      " 501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501.\n",
      " 501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501.\n",
      " 501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501.\n",
      " 501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501.\n",
      " 501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501.\n",
      " 501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501.\n",
      " 501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501.\n",
      " 501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501.\n",
      " 501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501.\n",
      " 501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501.\n",
      " 501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501.\n",
      " 501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501.\n",
      " 501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501.\n",
      " 501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501. 501.\n",
      " 501. 501. 501. 501. 501. 501.]\n",
      "[29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29.\n",
      " 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29.\n",
      " 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29.\n",
      " 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29.\n",
      " 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29.\n",
      " 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29.\n",
      " 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29.\n",
      " 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29.\n",
      " 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29.\n",
      " 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29.\n",
      " 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29.\n",
      " 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29.\n",
      " 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29.\n",
      " 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29.\n",
      " 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29.\n",
      " 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29.\n",
      " 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29. 29.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[-1.67754509e+00  2.33852052e+00  2.46546856e+01  3.50934853e+00\n",
      " -1.65560011e+00  1.20868096e+01  2.29907868e+01 -1.85046117e+00\n",
      " -8.90102153e-01 -4.79421998e+00 -1.21471949e+00 -1.08994310e+00\n",
      " -3.33840884e-01  2.25501055e+00  1.23076095e+00 -5.31973659e+00\n",
      " -2.21294240e-01  5.76784028e+00 -1.56181808e+01  1.22268429e+01\n",
      " -3.06605828e+00  2.44561457e+00  2.96086945e+00 -9.32098616e-01\n",
      " -1.30464095e+01  2.56769392e+00 -4.84598398e+00 -1.88875836e+00\n",
      "  1.81645393e-01  5.97092509e+00 -2.74228872e+00 -5.56834552e+00\n",
      "  2.33515894e+00  1.69456367e+00  7.30156320e+00  1.45493940e+00\n",
      "  4.71524417e+00 -1.80888787e+00 -9.54456739e-01  3.04245799e+00\n",
      " -5.62071668e+00 -4.26526067e+00 -2.65862320e+00  6.91444360e+00\n",
      " -3.10961357e+00  2.03361098e+01 -9.81255753e+00  1.68702386e+00\n",
      " -3.30081017e+01  1.10233541e+01  4.82149471e+00 -7.81553008e-01\n",
      "  2.97444454e+00  6.96272458e+00 -2.99342634e+01  2.23085516e+00\n",
      "  1.56873696e+01  3.13099440e-01  1.70694141e+00  3.51668296e+00\n",
      "  4.94071249e+00 -1.54039012e+01 -7.89974940e+00  7.08097787e-01\n",
      "  7.74574859e-01  4.29290777e-01 -6.81077329e-02 -1.20284876e+01\n",
      " -1.02398195e+00 -5.44501700e+00 -3.15352394e+00 -1.20238533e+01\n",
      " -2.86425957e+00 -6.88418576e+00  6.03617020e+00 -4.69986511e+00\n",
      " -9.06224973e-01 -5.46788484e+00 -8.24301773e+00 -7.39968572e-01\n",
      "  1.13154235e-01 -5.54116889e+00 -1.06641780e+02  8.12857363e-02\n",
      "  4.97699441e+00  1.46714159e-02 -4.57969767e+00  2.57133614e+00\n",
      "  7.57450255e+00  4.98278747e+00  2.73339840e+00  3.94762961e+00\n",
      " -4.80535939e-01  1.04196231e+01 -2.59916949e+00  9.30552282e+01\n",
      "  4.49083974e-01  5.51546948e+00  3.49498627e+00  2.19893075e+00\n",
      " -6.56012834e+00  3.96778948e-01  2.75636937e+00  6.93582461e-01\n",
      "  4.82767530e+00  3.10735043e-02  6.35408937e+00  1.75618369e+00\n",
      "  3.35015239e+01  1.56555896e+00 -1.12347014e+01  2.68545886e-02\n",
      "  1.82097229e+00 -6.47512570e+00 -5.31970206e+00 -4.74612636e+00\n",
      " -2.02662552e+01 -4.32222835e+00  1.03014588e+00 -1.99629204e+02\n",
      "  4.01147406e+00 -3.08683583e-01  1.66378348e+00  1.47289261e+00\n",
      " -1.63784423e+00  7.03733927e-02  1.28871443e+01 -3.60413817e+00\n",
      " -3.99791830e+00 -6.25294883e+00 -3.75063318e+00  3.06602071e+00\n",
      "  4.42780647e+00  3.32085596e+00 -6.36966650e-01  3.73784262e+00\n",
      "  2.17406609e+00  2.08595062e-01  3.97790708e+01  8.26134186e+00\n",
      "  5.44323716e+00 -3.46439943e+00 -1.20664099e+01 -6.45118931e+00\n",
      " -9.14885230e+00 -7.56537534e-01 -1.70076424e+00 -6.83213788e+00\n",
      "  3.43260142e+00  2.00983564e+00  5.79555582e+00 -1.03232107e+01\n",
      " -1.31140408e+01 -5.05312097e+01  4.07848659e+00 -2.43177981e+00\n",
      " -5.37731727e+00 -1.46101355e+01 -7.07501630e+00  1.53117067e+00\n",
      "  1.70123491e+00 -1.29789346e+01 -7.94862898e+00 -5.29395579e+00\n",
      " -1.17485235e+01 -6.25880665e+00  6.49873569e+00  1.30515306e+00\n",
      " -8.06741452e-01  3.40147809e+00 -2.90270653e-01  6.20373455e+00\n",
      " -2.39696436e+00 -2.14809209e+00  3.21664719e+00  4.10078200e+00\n",
      " -4.66335834e+01  5.54110865e+00 -6.86205057e+00  2.86542707e+00\n",
      " -1.18939547e+01 -2.13123427e+00  1.49241296e+00 -1.68495024e+01\n",
      " -3.37213128e+01 -6.52196809e+00 -5.17950990e-01  1.16544573e+01\n",
      "  1.18431723e+00 -3.71815016e-01  2.11706851e+00 -1.68176161e+00\n",
      " -2.65159759e+01 -2.96031904e+00 -2.98154417e+00  5.12285251e+00\n",
      "  1.37891142e+01 -8.96185234e+00 -5.36557120e+00  3.59300969e+00\n",
      " -1.37292615e+01 -1.22005318e+00 -2.33167393e-01  4.70479648e-01\n",
      "  5.53643414e+00  8.65005923e+00  5.03102333e+00  4.40045250e+00\n",
      " -3.75410971e+00  4.08023857e+00 -1.43107481e+02  7.88985405e-01\n",
      " -7.07258565e+00  1.95408315e+00 -4.13206945e+00  2.32219527e+00\n",
      "  1.30761628e+01 -4.36676445e+00 -6.46780117e+00  5.72065994e-01\n",
      "  3.37988171e+00 -1.37757994e+00  3.47348932e+01 -3.69087867e+00\n",
      "  2.96856226e+01  6.69737971e+00  6.28290241e+00  2.28083898e+01\n",
      "  9.29459595e+00 -2.44282912e+00 -6.60101526e-01 -9.40087684e-02\n",
      " -4.47815002e-01  1.22526599e+00 -9.35828952e-01 -6.58282635e+00\n",
      " -8.27985803e+00  6.35350238e-01 -3.72634573e+00 -1.35485530e+00\n",
      " -4.09950214e+00  3.40145886e-01  2.11266897e+00 -1.23157650e+01\n",
      "  3.45730272e+00 -7.50774540e+00  3.72124246e+00  3.98176422e+00\n",
      " -4.03003995e+00 -9.24988199e+00  3.62484829e+01 -4.64954167e+00\n",
      " -2.23982800e+00 -8.41509150e+00 -6.51571921e+00 -2.33712038e+01\n",
      " -1.46237631e-01  5.25670054e+00 -3.28658505e+00 -5.18004427e-01\n",
      "  4.83705645e+00 -6.53018603e+00  3.62259840e+00  4.62670915e+00\n",
      "  8.88713447e+00  1.26252620e+00 -5.82905843e-01  3.63792178e+00\n",
      " -4.63164752e+00  2.12398624e+00  2.93486923e-01 -1.21878601e+00\n",
      " -8.71148692e+00  5.19017029e-01 -8.85682343e-01 -3.61548492e+00\n",
      " -8.16830512e+00 -3.33040886e+00 -8.11738890e-01  2.95011789e+00\n",
      "  7.52846344e+00  7.30600778e+00 -5.07834348e-01  4.80263289e+00\n",
      "  1.35407936e+00 -2.07608885e+00 -3.44089683e+00 -3.47633391e+00\n",
      " -3.08750475e+00 -4.77258177e+00 -9.32623344e+00 -2.63089075e+00\n",
      "  1.05692324e+01  1.09042750e-01  3.57833570e+00  2.36970206e+01\n",
      " -1.28424133e+01 -6.27714906e+00 -7.90818845e-01 -1.93258986e+00]\n",
      "[-1.67754509e+00  2.33852052e+00  2.46546856e+01  3.50934853e+00\n",
      " -1.65560011e+00  1.20868096e+01  2.29907868e+01 -1.85046117e+00\n",
      " -8.90102153e-01 -4.79421998e+00 -1.21471949e+00 -1.08994310e+00\n",
      " -3.33840884e-01  2.25501055e+00  1.23076095e+00 -5.31973659e+00\n",
      " -2.21294240e-01  5.76784028e+00 -1.56181808e+01  1.22268429e+01\n",
      " -3.06605828e+00  2.44561457e+00  2.96086945e+00 -9.32098616e-01\n",
      " -1.30464095e+01  2.56769392e+00 -4.84598398e+00 -1.88875836e+00\n",
      "  1.81645393e-01  5.97092509e+00 -2.74228872e+00 -5.56834552e+00\n",
      "  2.33515894e+00  1.69456367e+00  7.30156320e+00  1.45493940e+00\n",
      "  4.71524417e+00 -1.80888787e+00 -9.54456739e-01  3.04245799e+00\n",
      " -5.62071668e+00 -4.26526067e+00 -2.65862320e+00  6.91444360e+00\n",
      " -3.10961357e+00  2.03361098e+01 -9.81255753e+00  1.68702386e+00\n",
      " -3.30081017e+01  1.10233541e+01  4.82149471e+00 -7.81553008e-01\n",
      "  2.97444454e+00  6.96272458e+00 -2.99342634e+01  2.23085516e+00\n",
      "  1.56873696e+01  3.13099440e-01  1.70694141e+00  3.51668296e+00\n",
      "  4.94071249e+00 -1.54039012e+01 -7.89974940e+00  7.08097787e-01\n",
      "  7.74574859e-01  4.29290777e-01 -6.81077329e-02 -1.20284876e+01\n",
      " -1.02398195e+00 -5.44501700e+00 -3.15352394e+00 -1.20238533e+01\n",
      " -2.86425957e+00 -6.88418576e+00  6.03617020e+00 -4.69986511e+00\n",
      " -9.06224973e-01 -5.46788484e+00 -8.24301773e+00 -7.39968572e-01\n",
      "  1.13154235e-01 -5.54116889e+00 -1.06641780e+02  8.12857363e-02\n",
      "  4.97699441e+00  1.46714159e-02 -4.57969767e+00  2.57133614e+00\n",
      "  7.57450255e+00  4.98278747e+00  2.73339840e+00  3.94762961e+00\n",
      " -4.80535939e-01  1.04196231e+01 -2.59916949e+00  9.30552282e+01\n",
      "  4.49083974e-01  5.51546948e+00  3.49498627e+00  2.19893075e+00\n",
      " -6.56012834e+00  3.96778948e-01  2.75636937e+00  6.93582461e-01\n",
      "  4.82767530e+00  3.10735043e-02  6.35408937e+00  1.75618369e+00\n",
      "  3.35015239e+01  1.56555896e+00 -1.12347014e+01  2.68545886e-02\n",
      "  1.82097229e+00 -6.47512570e+00 -5.31970206e+00 -4.74612636e+00\n",
      " -2.02662552e+01 -4.32222835e+00  1.03014588e+00 -1.99629204e+02\n",
      "  4.01147406e+00 -3.08683583e-01  1.66378348e+00  1.47289261e+00\n",
      " -1.63784423e+00  7.03733927e-02  1.28871443e+01 -3.60413817e+00\n",
      " -3.99791830e+00 -6.25294883e+00 -3.75063318e+00  3.06602071e+00\n",
      "  4.42780647e+00  3.32085596e+00 -6.36966650e-01  3.73784262e+00\n",
      "  2.17406609e+00  2.08595062e-01  3.97790708e+01  8.26134186e+00\n",
      "  5.44323716e+00 -3.46439943e+00 -1.20664099e+01 -6.45118931e+00\n",
      " -9.14885230e+00 -7.56537534e-01 -1.70076424e+00 -6.83213788e+00\n",
      "  3.43260142e+00  2.00983564e+00  5.79555582e+00 -1.03232107e+01\n",
      " -1.31140408e+01 -5.05312097e+01  4.07848659e+00 -2.43177981e+00\n",
      " -5.37731727e+00 -1.46101355e+01 -7.07501630e+00  1.53117067e+00\n",
      "  1.70123491e+00 -1.29789346e+01 -7.94862898e+00 -5.29395579e+00\n",
      " -1.17485235e+01 -6.25880665e+00  6.49873569e+00  1.30515306e+00\n",
      " -8.06741452e-01  3.40147809e+00 -2.90270653e-01  6.20373455e+00\n",
      " -2.39696436e+00 -2.14809209e+00  3.21664719e+00  4.10078200e+00\n",
      " -4.66335834e+01  5.54110865e+00 -6.86205057e+00  2.86542707e+00\n",
      " -1.18939547e+01 -2.13123427e+00  1.49241296e+00 -1.68495024e+01\n",
      " -3.37213128e+01 -6.52196809e+00 -5.17950990e-01  1.16544573e+01\n",
      "  1.18431723e+00 -3.71815016e-01  2.11706851e+00 -1.68176161e+00\n",
      " -2.65159759e+01 -2.96031904e+00 -2.98154417e+00  5.12285251e+00\n",
      "  1.37891142e+01 -8.96185234e+00 -5.36557120e+00  3.59300969e+00\n",
      " -1.37292615e+01 -1.22005318e+00 -2.33167393e-01  4.70479648e-01\n",
      "  5.53643414e+00  8.65005923e+00  5.03102333e+00  4.40045250e+00\n",
      " -3.75410971e+00  4.08023857e+00 -1.43107481e+02  7.88985405e-01\n",
      " -7.07258565e+00  1.95408315e+00 -4.13206945e+00  2.32219527e+00\n",
      "  1.30761628e+01 -4.36676445e+00 -6.46780117e+00  5.72065994e-01\n",
      "  3.37988171e+00 -1.37757994e+00  3.47348932e+01 -3.69087867e+00\n",
      "  2.96856226e+01  6.69737971e+00  6.28290241e+00  2.28083898e+01\n",
      "  9.29459595e+00 -2.44282912e+00 -6.60101526e-01 -9.40087684e-02\n",
      " -4.47815002e-01  1.22526599e+00 -9.35828952e-01 -6.58282635e+00\n",
      " -8.27985803e+00  6.35350238e-01 -3.72634573e+00 -1.35485530e+00\n",
      " -4.09950214e+00  3.40145886e-01  2.11266897e+00 -1.23157650e+01\n",
      "  3.45730272e+00 -7.50774540e+00  3.72124246e+00  3.98176422e+00\n",
      " -4.03003995e+00 -9.24988199e+00  3.62484829e+01 -4.64954167e+00\n",
      " -2.23982800e+00 -8.41509150e+00 -6.51571921e+00 -2.33712038e+01\n",
      " -1.46237631e-01  5.25670054e+00 -3.28658505e+00 -5.18004427e-01\n",
      "  4.83705645e+00 -6.53018603e+00  3.62259840e+00  4.62670915e+00\n",
      "  8.88713447e+00  1.26252620e+00 -5.82905843e-01  3.63792178e+00\n",
      " -4.63164752e+00  2.12398624e+00  2.93486923e-01 -1.21878601e+00\n",
      " -8.71148692e+00  5.19017029e-01 -8.85682343e-01 -3.61548492e+00\n",
      " -8.16830512e+00 -3.33040886e+00 -8.11738890e-01  2.95011789e+00\n",
      "  7.52846344e+00  7.30600778e+00 -5.07834348e-01  4.80263289e+00\n",
      "  1.35407936e+00 -2.07608885e+00 -3.44089683e+00 -3.47633391e+00\n",
      " -3.08750475e+00 -4.77258177e+00 -9.32623344e+00 -2.63089075e+00\n",
      "  1.05692324e+01  1.09042750e-01  3.57833570e+00  2.36970206e+01\n",
      " -1.28424133e+01 -6.27714906e+00 -7.90818845e-01 -1.93258986e+00]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[-1.86263568e-01 -5.83668672e-02  4.84632724e-02  5.09335287e-02\n",
      " -6.17389209e-02  1.26213003e-01  1.57256313e-01  3.03935632e-02\n",
      "  8.17905739e-02 -6.84328787e-02  1.64613694e-01  1.08844448e-01\n",
      " -3.85384618e-02  3.77571490e-02 -3.49863772e-02  6.12170808e-02\n",
      " -1.85804240e-01 -5.23099108e-02  6.58901129e-02  6.53479050e-02\n",
      " -1.11263216e-01  1.40838560e-01 -3.31927711e-02  2.11821580e-02\n",
      " -4.06940460e-01  4.41294197e-02 -1.82572972e-01  1.73848197e-02\n",
      " -6.38114475e-02 -7.11665619e-02  8.41382891e-03 -3.94135974e-02\n",
      " -5.28614819e-02  1.18106375e-01  2.05673017e-01  1.84444156e-01\n",
      "  5.25268712e-02 -7.45958015e-02 -9.99469552e-02  3.96410404e-02\n",
      "  1.23023009e-01 -1.97533697e-01  8.52919370e-03 -4.52800575e-02\n",
      "  1.02054663e-01  1.16197020e-01 -4.63555604e-02  1.28792517e-01\n",
      " -2.82676022e-01  5.44488784e-02  1.01189036e-02  1.30500085e-01\n",
      " -6.65011443e-02  1.55503079e-01  1.00886572e-01  1.55447904e-01\n",
      " -1.13006875e-01  3.24204369e-02 -1.99311126e-01 -1.16308678e-01\n",
      " -9.75600779e-02  7.16091692e-02 -3.81555495e-02 -3.54690989e-02\n",
      " -7.19364807e-02  7.73767754e-02  8.41509840e-02 -2.62386389e-02\n",
      "  1.61929280e-02  1.04634594e-01 -8.16232581e-02  4.01088586e-02\n",
      "  2.06022090e-02  7.74971442e-02 -6.98514357e-02 -6.69213459e-02\n",
      " -7.03617139e-02  1.17059954e-01  9.26995154e-02 -4.02870886e-02\n",
      "  1.09112803e-01 -2.24575922e-02 -6.27224284e-02 -1.46962568e-01\n",
      " -5.96805215e-02 -5.19504799e-02  2.88519263e-03 -2.12604932e-01\n",
      " -5.02309874e-02 -5.40961986e-02  1.66410595e-01  1.19694684e-01\n",
      "  6.37759715e-02  4.35188692e-03  7.54417600e-02  3.15322325e-01\n",
      "  1.57998428e-01 -3.08503453e-02 -1.23892110e-02 -1.31510247e-01\n",
      "  4.29020077e-03 -3.20024369e-02  4.46300309e-02 -4.15316829e-02\n",
      "  2.98308404e-02  7.28435339e-02 -1.43664557e-01 -8.76450529e-02\n",
      " -6.37406681e-02 -1.08232265e-01 -5.89628965e-02 -1.38051009e-01\n",
      "  2.25104488e-01 -2.25499123e-02  6.76987087e-03  8.52152742e-02\n",
      " -1.70660943e-01 -1.46329679e-01  4.27910946e-02 -3.99859637e-01\n",
      "  8.74725282e-02 -6.89150486e-02  8.83214436e-02 -1.06960721e-01\n",
      "  1.24809717e-01 -2.06134543e-02  1.27200726e-02  2.49019526e-02\n",
      "  6.37913495e-03  4.76675220e-02 -9.14633907e-02 -8.50211307e-02\n",
      "  7.38238953e-02  1.21803023e-01 -7.48791769e-02  1.68974875e-01\n",
      " -8.68801028e-04 -7.86420535e-02 -9.13969539e-02 -1.80870496e-01\n",
      " -1.49164032e-01  5.68302050e-02  2.88632154e-01 -1.95943195e-01\n",
      "  1.75292037e-01  7.11771082e-02  6.66259360e-02 -1.29563443e-01\n",
      " -1.97896330e-02 -3.37678958e-02 -9.26917437e-02  1.11884743e-01\n",
      "  7.64656737e-02 -1.56129509e-01 -6.82495162e-02  1.75388050e-02\n",
      " -5.21903858e-02  8.10147226e-02 -9.25353418e-03  1.56606976e-02\n",
      "  3.85615993e-02 -5.74545041e-02 -2.94714794e-02 -3.47358377e-02\n",
      " -3.47633101e-02  1.35193354e-01  7.69135486e-02  1.18195046e-01\n",
      " -3.55310291e-02  8.77152663e-03  8.22705654e-02 -1.18260384e-02\n",
      "  1.55407314e-02  2.79739872e-03  8.69455207e-02  3.68889612e-02\n",
      "  4.28713141e-02  1.92083395e-02  4.78226664e-02  2.89551634e-03\n",
      " -5.15160275e-02 -1.16274223e-01  2.86726644e-02 -1.46710429e-01\n",
      " -1.11009242e-01 -1.26290809e-01  1.48398513e-01 -2.40005448e-02\n",
      "  4.61331755e-02  1.75837856e-02  6.90626521e-02  8.46103020e-03\n",
      " -8.64759795e-02 -4.83432412e-03  1.21474103e-01 -2.35644083e-01\n",
      "  5.31211831e-02  1.38261719e-02  1.79776885e-02  3.97715967e-02\n",
      " -1.76692121e-01 -2.78067596e-01  1.94804482e-02 -1.22630063e-01\n",
      " -9.02177487e-02  1.15218558e-02 -1.30633621e-01  1.56646032e-01\n",
      " -4.12130803e-02  1.40115768e-01 -3.73471014e-01  2.78406225e-02\n",
      " -1.28520649e-01  2.83222646e-04 -6.48596282e-02  1.13688013e-01\n",
      "  8.81557446e-02  1.05411615e-02 -2.11998932e-02 -7.62200598e-02\n",
      " -6.29416779e-02  1.46315900e-01 -2.96327047e-01 -1.10315243e-02\n",
      "  3.10415402e-01  5.88801149e-02  2.74097864e-02 -7.78450174e-02\n",
      "  7.01342653e-02  4.82159667e-02 -1.22708565e-01  1.06978443e-01\n",
      " -8.28267261e-03 -1.83977969e-02  4.41470183e-03 -1.71248054e-01\n",
      " -3.26157808e-02 -6.88011572e-03  3.22891269e-02 -2.29171577e-01\n",
      " -3.38695049e-02 -5.94173018e-02  5.43495240e-02  6.12790305e-02\n",
      " -2.90728353e-01  7.28598274e-02 -1.41028171e-01  8.70732968e-02\n",
      " -1.10173303e-01 -1.05211552e-01  1.37754006e-01  8.55513290e-03\n",
      " -2.28760332e-01  1.29775144e-02  1.40902877e-01  1.61480781e-01\n",
      " -1.70346249e-01 -8.17980878e-02 -6.21802025e-02 -1.33906501e-01\n",
      "  1.77113611e-01  1.57661892e-01  1.25612956e-01  4.90553714e-02\n",
      "  1.31319847e-01  1.07744839e-02  6.03689179e-02 -8.12902302e-02\n",
      "  4.11468856e-02  1.00966478e-01  5.94634213e-02  1.21903423e-01\n",
      "  1.37042198e-01 -2.15637311e-03  1.36647027e-01 -5.81357572e-02\n",
      "  1.49020318e-01 -1.61138263e-01  5.67235295e-02  6.89834356e-03\n",
      " -1.40351856e-01 -1.02007920e-01  4.92770439e-02  5.84468246e-03\n",
      " -7.23153828e-02  3.88353765e-02 -9.84327495e-02 -2.30399761e-02\n",
      "  5.47045600e-02 -5.67083294e-02  3.73706669e-02 -1.12180687e-01\n",
      " -4.57682293e-02 -1.17430039e-01 -5.35975643e-02  7.20626190e-02\n",
      "  1.49082379e-01  6.03451561e-02 -1.56328201e-01  7.50091523e-02]\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "[-7.10238465e-01 -1.33099094e-01 -3.83091813e-01  6.31520932e-02\n",
      "  8.90692187e-02  9.12416642e-01 -6.29691861e-03  4.09514262e-02\n",
      "  2.33171734e-01 -3.09705131e-01  6.32450501e-02  2.07712598e-01\n",
      " -3.14649047e-02  7.03384729e-02  1.23037230e-01  1.08412176e-01\n",
      "  2.30617732e-02 -2.73818974e-02 -1.75620845e-02 -8.17900989e-03\n",
      " -2.30632796e-01 -9.51153652e-02  5.03948290e-02 -3.68342292e-02\n",
      " -1.33978213e+00  2.03141095e-01 -2.93277098e-01  9.42677594e-03\n",
      " -7.87519817e-02 -1.37920427e-01 -1.27489082e-01 -8.81342591e-02\n",
      "  6.04488089e-02  7.34742116e-02  6.49388503e-02  3.00249619e-01\n",
      " -8.12961017e-02  1.36126497e-02  1.83423379e-02  1.45834632e-01\n",
      " -2.16256169e-01  3.55325908e-01  1.73115767e-01  1.30817433e-01\n",
      "  1.81688038e-01  1.94976373e-01 -2.59509502e-02  4.16940633e-02\n",
      " -1.06481176e+00  1.37863403e-01  1.41176106e-01  1.97708314e-01\n",
      "  3.20418521e-02  1.62808884e-01  6.28576496e-01  9.85322581e-02\n",
      "  1.50446026e-01 -3.13162930e-02 -2.59909837e-04 -5.73947405e-02\n",
      "  2.41842194e-02  3.65700485e-01 -1.62217201e-01  8.31736858e-02\n",
      " -9.63459390e-02  8.19199278e-02  2.13704034e-01 -1.62625993e-01\n",
      "  6.10468282e-02 -9.82699220e-02 -6.23782046e-02  3.13119132e-01\n",
      " -1.10937117e-01  9.28914300e-02 -2.32838594e-01  4.05912251e-02\n",
      " -7.10280315e-02  2.39511266e-01 -1.81127116e-01 -1.98532082e-01\n",
      "  2.99197627e-01 -1.85859990e-01 -1.65325040e+00 -8.33495722e-02\n",
      " -1.00780452e-01  2.47816031e-03  4.13544109e-02 -2.53605580e-01\n",
      " -1.46267561e-01  5.33442774e-02  2.82525618e-01 -9.14646473e-02\n",
      " -9.70221655e-02  1.15951069e-01 -3.81083216e-02  7.81763195e-01\n",
      "  2.07283026e-01  1.08908033e-01 -6.56381001e-02 -2.93405428e-01\n",
      "  3.51493608e-02  7.27828872e-02 -1.92646933e-02  1.03977232e-01\n",
      " -1.25548170e-01  1.03211799e-01 -3.41573913e-01  2.55839946e-03\n",
      " -6.82730753e-01 -1.90138840e-01  8.15847731e-02 -1.70203297e-01\n",
      "  1.14860816e-01  1.84094979e-01  2.50093726e-01  4.86364037e-02\n",
      " -7.73674632e-02 -8.21796840e-02  1.97179658e-01 -2.99168258e+00\n",
      "  1.26411694e-01 -2.19449732e-01  1.63256397e-01  1.31103722e-01\n",
      "  6.33833579e-02  5.87973085e-02  2.55864096e-01  1.61452302e-01\n",
      "  1.19219060e-01  3.34674097e-02 -9.61878763e-02 -5.79399583e-02\n",
      " -6.61345947e-02  2.69718063e-01 -1.09892249e-01  7.46170706e-02\n",
      "  2.77688697e-01 -1.23251734e-01  5.54373871e-01 -3.93896025e-01\n",
      " -8.05455482e-02 -8.18212847e-02  1.34787721e-01 -6.02172304e-01\n",
      "  1.23351776e-01 -1.58191142e-02  4.60761758e-02 -2.31685150e-01\n",
      " -1.96959401e-02 -6.98740956e-02 -2.60996731e-02  4.49851308e-01\n",
      " -5.83470648e-02 -1.20684139e+00 -2.08425139e-01  1.94193720e-01\n",
      " -5.23769760e-01  1.77914640e-01 -2.75776273e-01 -1.57584073e-01\n",
      " -7.36821789e-02 -5.23453681e-01 -3.66650639e-02  1.49692654e-01\n",
      "  1.11948376e-01 -1.01051130e-01  2.82226480e-01  2.80599931e-01\n",
      " -1.03312037e-01 -1.39317018e-01  8.17215798e-02  1.13927463e-01\n",
      "  6.85142757e-02 -8.19873519e-02  2.63477315e-02  2.28007354e-01\n",
      " -1.00859547e+00  8.44788234e-03  8.06878220e-02 -7.49264308e-02\n",
      "  8.36561556e-02  5.55808471e-02 -6.23446092e-03 -6.75809908e-02\n",
      "  1.04374218e-01 -1.47074806e-01  1.89593225e-01 -2.69306424e-01\n",
      "  3.76507714e-02 -5.02808946e-02  8.64316020e-02 -5.48700053e-02\n",
      " -5.89035438e-01 -4.33002001e-02  1.73289272e-01 -2.27546244e-01\n",
      " -1.97850799e-01  1.16041677e-01 -3.15351835e-02 -1.13377508e-02\n",
      "  1.95579121e-01 -2.74117763e-01  1.17658846e-02 -7.43293357e-02\n",
      " -1.62176294e-01 -1.00918409e-02 -4.05630582e-02  1.73308953e-01\n",
      "  4.56482417e-02  2.13679376e-01 -2.49025594e+00  1.12286215e-01\n",
      " -1.12018565e-01  1.00436801e-02 -1.03132064e-01  9.99296920e-02\n",
      "  3.09089625e-02 -1.65275429e-01  3.09129692e-01 -1.20285466e-01\n",
      " -4.81900154e-02  2.87817435e-01  5.06043916e-01 -1.41862885e-01\n",
      "  3.10201435e-01 -2.05027846e-02  1.03838994e-01 -4.56968211e-02\n",
      "  1.47809724e-01  1.10471130e-01 -1.13142600e-01  1.28560074e-01\n",
      "  1.41758116e-01 -1.80667897e-01 -1.82930537e-03 -4.13586582e-01\n",
      " -1.43362227e-01  1.13394499e-01  8.71581340e-02 -3.56127898e-01\n",
      " -1.95515453e-01 -1.36848220e-02  1.72089938e-01 -3.18221408e-01\n",
      " -1.35876967e-01  1.69454486e-01  8.32004269e-02  7.01442863e-01\n",
      " -2.45662266e-01 -1.41942341e-01  3.86077121e-02  5.74069918e-02\n",
      " -1.72916851e-01  1.02663157e-01 -2.02474443e-01  1.13981851e-01\n",
      "  3.84087258e-02  3.00458798e-02 -1.66033493e-01 -7.28129645e-02\n",
      "  1.85612655e-01  2.27755796e-01  2.26425943e-01  2.45081291e-01\n",
      "  1.81806324e-01  1.26776247e-02  9.89592804e-02 -8.29660977e-02\n",
      " -1.69985818e-02 -1.15777622e-02 -8.44631171e-02  5.87493697e-02\n",
      "  2.05017006e-02  1.76615910e-01 -9.22208770e-02  4.41817505e-02\n",
      "  2.73328874e-01 -2.34815652e-01  1.19029933e-01  8.60530811e-02\n",
      " -5.30037009e-01 -8.52703002e-02  1.00037131e-01 -1.95074187e-02\n",
      " -1.06312918e-01  8.42423731e-03  5.41319458e-02  1.39653952e-01\n",
      " -7.90960763e-02  7.77579585e-02  1.77585174e-01 -2.80109924e-01\n",
      "  7.93862864e-01 -1.93761461e-01 -8.50657608e-02 -2.26887744e-01\n",
      "  7.97476500e-03 -1.74303893e-01  2.91234097e-02  1.41676438e-01]\n",
      "[ 4.37644916e-03 -2.65889365e-04  3.77513506e-02  1.92830060e-02\n",
      " -1.32732838e-03  4.83062677e-03  2.49291747e-03 -4.06035502e-03\n",
      "  2.03042291e-02 -2.71949614e-03 -9.70000960e-03 -1.13087427e-02\n",
      "  7.76361721e-03  1.67231616e-02  1.11479945e-02  8.93362158e-04\n",
      " -3.32172518e-03  1.80933648e-03  2.50277319e-03 -2.26723310e-03\n",
      " -3.49395387e-02  1.96370832e-03  2.66844430e-03  3.48050846e-03\n",
      " -2.79523134e-02  1.11609912e-02 -1.89965125e-02 -4.18241805e-04\n",
      " -7.77475443e-03 -2.08711624e-02 -1.28571726e-02 -2.21668999e-03\n",
      "  7.82884751e-03 -1.06493691e-02  7.16357399e-03  9.93961189e-03\n",
      "  8.88148788e-03  4.16510506e-03 -8.23093462e-04  8.05605389e-03\n",
      " -2.96769594e-03 -7.25060403e-02  2.73151090e-03 -1.16981845e-02\n",
      "  7.58485257e-05  2.36280728e-02 -1.01287859e-02  2.32290849e-03\n",
      " -1.76081825e-02 -5.88563969e-03 -5.58501866e-04  3.55261727e-03\n",
      "  6.52176607e-03  7.08353007e-03 -8.42398405e-03 -1.05353547e-02\n",
      " -1.42838694e-02 -8.16552807e-03 -2.60391571e-02 -1.42536443e-02\n",
      "  3.02629243e-03 -1.91784818e-02 -1.15492241e-02 -6.63331710e-03\n",
      " -1.79280981e-03  3.38458968e-03 -7.42085231e-03 -4.68727909e-02\n",
      "  1.23121375e-02 -2.78547313e-03  1.03885708e-02 -2.28945026e-03\n",
      " -2.54683197e-03 -2.17374717e-03  2.46925163e-04  1.28703434e-02\n",
      "  2.53708707e-03 -1.70501210e-02  1.58834830e-02 -6.63235039e-03\n",
      " -3.83685878e-03  7.87188392e-03 -8.79941694e-03 -2.79407459e-03\n",
      "  8.31938256e-03 -6.13977481e-03 -1.21280039e-03 -1.52718071e-02\n",
      " -1.89481897e-03  3.81523417e-03 -2.82209907e-02  5.46444766e-03\n",
      " -4.03415188e-02  4.62462753e-02 -1.13394614e-02  3.87212150e-02\n",
      "  3.79813905e-03  4.02859849e-04  2.31676502e-03  4.87888372e-03\n",
      " -5.69145102e-03  4.57920833e-03  5.23002353e-03  1.43647883e-02\n",
      "  2.62319902e-03 -1.15246195e-02 -5.85125200e-03 -1.73456408e-03\n",
      " -4.35378449e-03  1.06820278e-02 -1.36377639e-03 -5.68721956e-03\n",
      "  3.47426627e-03 -1.21558015e-03  2.43331934e-03 -7.93095212e-03\n",
      " -1.63726404e-03  1.40473247e-02  9.52095352e-03 -2.48352960e-02\n",
      " -8.19651969e-03 -1.09512536e-02  3.69450456e-04 -6.49096328e-04\n",
      "  1.53706530e-02  4.13609017e-03 -2.29947851e-03 -8.63115117e-03\n",
      "  3.41077452e-03  6.83231861e-04  5.78607107e-03  1.83972511e-02\n",
      "  1.65695082e-02  4.42042835e-02 -8.47932789e-03 -4.01847297e-03\n",
      "  1.45450281e-02  2.70204339e-03 -5.78787783e-03 -3.67186940e-03\n",
      " -3.01710935e-03 -1.65825023e-03  4.39149290e-02 -2.43854001e-02\n",
      " -2.83396640e-03  1.89589115e-03 -8.01484752e-03 -3.03949956e-02\n",
      " -7.24785216e-03  1.42255216e-03  3.22208460e-03  2.80179884e-02\n",
      " -6.14748895e-03 -9.08983685e-03 -2.43206415e-02  2.62015103e-03\n",
      "  1.73896775e-02 -3.50433998e-02  3.98795642e-02  1.08450484e-02\n",
      " -7.66075309e-03 -1.62426531e-02  6.69040205e-03 -1.34993764e-03\n",
      " -5.15332399e-03 -6.74049079e-05 -6.37222920e-03  9.90397390e-03\n",
      "  1.86199006e-02  3.29667423e-03  4.55421768e-03 -4.23257705e-03\n",
      " -1.37090194e-03  3.23280040e-03 -3.55740916e-03  8.58601369e-03\n",
      " -7.38786720e-03  1.37835126e-02  1.71274564e-03  1.51346130e-05\n",
      " -1.18628889e-02 -9.42252390e-03 -4.70064860e-03 -2.61376780e-02\n",
      " -2.77297422e-02 -6.62052445e-03 -4.16160241e-04  1.73334293e-02\n",
      "  9.42442473e-03 -1.46202845e-02 -1.03945341e-02  5.81708620e-04\n",
      " -3.11713256e-02  1.46952152e-04 -6.14496937e-04  1.20581291e-03\n",
      "  2.97397673e-02 -1.40466653e-02 -1.45198614e-03  7.25159654e-03\n",
      " -3.51074338e-02 -7.63376011e-03  7.88704120e-03  3.70531016e-05\n",
      "  9.10958834e-03  2.01383326e-02  8.53373576e-03  1.09669855e-02\n",
      "  2.85185338e-03  4.19712719e-03  2.08920445e-02 -3.51245049e-03\n",
      "  3.76191847e-02 -2.21614167e-02 -2.42890394e-03 -3.39068263e-03\n",
      "  9.00939922e-04 -4.95417323e-03 -2.56722374e-03  9.66803462e-04\n",
      "  2.36748252e-03 -5.89736598e-03 -1.78922154e-02 -1.81925180e-03\n",
      "  4.70772982e-02  1.03865480e-02  3.02842539e-03  3.51433866e-02\n",
      " -7.66831776e-03  5.20596374e-03 -7.65543897e-03 -1.33543173e-02\n",
      "  4.53438703e-03  8.45737103e-03  6.63609197e-03  3.26968953e-02\n",
      "  1.20951803e-02  1.11292256e-02  1.22317160e-02 -3.82066914e-03\n",
      "  1.05540249e-02 -6.25356217e-04 -5.17142331e-03 -1.34157846e-02\n",
      " -1.70966014e-02  9.77976713e-03  2.75636022e-03 -1.35598956e-02\n",
      " -2.58975226e-04 -9.24804993e-03  2.02200375e-02  1.19449873e-03\n",
      "  5.56292245e-03  6.47211634e-03  2.15082895e-02  2.04419736e-02\n",
      "  1.45185669e-03 -4.36588377e-03  4.31994069e-03  3.51089705e-03\n",
      " -8.55834596e-03  7.92593602e-03  7.53890490e-03 -4.91242297e-03\n",
      "  7.33305700e-03  4.99933865e-03  6.65420573e-03 -9.40022338e-03\n",
      "  1.30817713e-02  6.92654261e-03  1.88395865e-02 -2.80127162e-03\n",
      "  3.19648022e-03 -3.97537882e-03  5.94350090e-03  4.13384475e-03\n",
      "  9.23638977e-03  1.30927917e-02 -6.17632456e-03 -1.63325341e-03\n",
      " -4.51645479e-02  2.24177609e-03  8.32073018e-03  3.29456758e-03\n",
      " -6.22643111e-03  3.63509520e-03 -1.48080103e-02  3.41132493e-03\n",
      "  3.23069049e-03  1.25224609e-02  2.47319993e-02  9.94043984e-03\n",
      " -1.13624400e-02 -7.86816981e-03  1.12142349e-02  3.06946952e-02\n",
      "  4.20507528e-02  6.91766990e-03  2.73172464e-03  9.11587104e-03]\n",
      "[4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4.\n",
      " 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4.\n",
      " 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4.\n",
      " 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4.\n",
      " 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4.\n",
      " 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4.\n",
      " 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4.\n",
      " 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4.\n",
      " 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4.\n",
      " 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4.\n",
      " 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4.\n",
      " 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4.\n",
      " 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4.]\n"
     ]
    }
   ],
   "source": [
    "for i in range(15):\n",
    "    print(data[0][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "\n",
    "new_data = [numpy.concatenate((datum[0][0], datum[1][0], datum[2][0], datum[3][0], datum[4][0], datum[5][0] )) for datum in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "\n",
    "new_data = [numpy.concatenate((numpy.array([datum[0][0]]), numpy.array([datum[1][0]]), numpy.array([datum[2][0]]), numpy.array([datum[3][0]]), numpy.array([datum[4][0]]), numpy.array([datum[5][0]]), numpy.array([datum[6][0]]), numpy.array(datum[7]), numpy.array(datum[8]), numpy.array([datum[9][0]]), numpy.array(datum[10]), numpy.array([datum[11][0]]), numpy.array(datum[12]), numpy.array(datum[13]), numpy.array([datum[14][0]])), axis=None) for datum in data]\n",
    "\n",
    "print(numpy.array(new_data).shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(new_data, labels, test_size=0.2, random_state=1)\n",
    "\n",
    "data_dict = {\"x_tr\": X_train, \"x_te\": X_test, \"y_tr\": y_train, \"y_te\": y_test}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
