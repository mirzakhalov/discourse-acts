{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features setup correctly\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Bidirectional, Conv2D\n",
    "import numpy\n",
    "from gensim.models.fasttext import load_facebook_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from features import Features\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk import tokenize\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "\n",
    "trained_model = None\n",
    "f = Features()\n",
    "f.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "global fasttext_model\n",
    "\n",
    "discourse = ['other', 'agreement', 'announcement', 'appreciation', 'humor', 'answer', 'elaboration', 'negativereaction',\n",
    "             'question', 'disagreement']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fasttext():\n",
    "    global fasttext_model\n",
    "    fasttext_model = load_facebook_model('wiki-news-300d-1M-subword.bin')\n",
    "\n",
    "def get_data(filename):\n",
    "    load_json_data = []\n",
    "    count = 0\n",
    "    with open(filename) as jsonfile:\n",
    "        for line in jsonfile:\n",
    "            jline = json.loads(line)\n",
    "            load_json_data.append(jline)\n",
    "            count += 1\n",
    "    return load_json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(load_data):\n",
    "    global errors\n",
    "    global f\n",
    "    count = 0\n",
    "    count_no_author = 0\n",
    "    count_no_title = 0\n",
    "    process_data_list = []\n",
    "    process_label_list = []\n",
    "    for jline in load_data:\n",
    "        author = None\n",
    "        if 'author' in jline['posts'][0]:\n",
    "            author = jline['posts'][0]['author']\n",
    "        for post in jline['posts']:\n",
    "            try:\n",
    "                # Structure\n",
    "                features = f.getStructureFeatures(jline, post['id'])\n",
    "                # Content\n",
    "                if 'body' in post:\n",
    "                    features.append(fasttext_Vec(post['body']))\n",
    "                else:\n",
    "                    features.append(numpy.zeros(300))\n",
    "                    count_no_title += 1\n",
    "                    \n",
    "                # ge66t the vector for the parent body\n",
    "                features.append(fasttext_Vec(f.getParentBody(jline, post['id'])))\n",
    "                # Author\n",
    "                features.append(f.isSameAuthor(jline, post))\n",
    "                \n",
    "                if 'author' in post:\n",
    "                    features.append(fasttext_Vec(post['author']))\n",
    "                    \n",
    "                    if author == post['author']:\n",
    "                        features.append(numpy.full(300, 1.0))\n",
    "                    else:\n",
    "                        features.append(numpy.full(300, 0.0))\n",
    "                else:\n",
    "                    features.append(numpy.zeros(300))\n",
    "                    features.append(numpy.zeros(300))\n",
    "                    count_no_author += 1\n",
    "                \n",
    "                \n",
    "                \n",
    "                if 'title' in jline:\n",
    "                    features.append(fasttext_Vec(jline['title']))\n",
    "                else:\n",
    "                    features.append(numpy.zeros(300))\n",
    "                    \n",
    "                \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                # Community\n",
    "                if 'subreddit' in jline:\n",
    "                    features.append(fasttext_Vec(jline['subreddit']))\n",
    "                else:\n",
    "                    features.append(numpy.zeros(300))\n",
    "                    \n",
    "                # Thread\n",
    "                features += f.thread_info(jline)\n",
    "                feature_nparr = numpy.array(features)\n",
    "                label = discourse.index(post['majority_type'])\n",
    "                process_label_list.append([label])\n",
    "                process_data_list.append(feature_nparr)\n",
    "            except Exception as e:\n",
    "                count += 1\n",
    "    print(\"Total exception count: \" + str(count))\n",
    "    print(\"No authors: \" + str(count_no_author))\n",
    "    print(\"No titles: \" + str(count_no_title))\n",
    "    process_data_list = numpy.array(process_data_list)\n",
    "    process_label_list = numpy.array(process_label_list)\n",
    "    print(process_data_list.shape)\n",
    "    print(process_label_list.shape)\n",
    "    print(\"done processing\")\n",
    "    return process_data_list, process_label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def set_model(data):\n",
    "    global trained_model\n",
    "    input_shape = data[0].shape\n",
    "    # After trying a bunch of different methods, this one worked the best\n",
    "    t_model = tf.keras.Sequential([\n",
    "        \n",
    "        Bidirectional(LSTM(100, input_shape=input_shape)),\n",
    "        #Bidirectional(LSTM(100)),\n",
    "        #Bidirectional(LSTM(64)),\n",
    "        #hub_layer,\n",
    "        Dropout(0.5),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.25),\n",
    "        Dense(32, activation='relu'),\n",
    "        \n",
    "        #tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    t_model.compile(optimizer='rmsprop',\n",
    "                    loss='categorical_crossentropy',\n",
    "                    metrics=['categorical_accuracy'])\n",
    "    trained_model = t_model\n",
    "\n",
    "def train(data, labels):\n",
    "    global trained_model\n",
    "    checkpoint_path = 'training_1/cp.ckpt'\n",
    "    set_model(data)\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, save_weights_only=True, verbose=1)\n",
    "    trained_model.fit(data, to_categorical(labels), validation_split=0.1, epochs=10, batch_size=128, callbacks=[cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fasttext_Vec(body):\n",
    "    global fasttext_model\n",
    "    global lemmatizer\n",
    "    global tokenizer\n",
    "    tokens = tokenizer.tokenize(body)\n",
    "    output = numpy.zeros(300)\n",
    "    for token in tokens:\n",
    "        try:\n",
    "            output = numpy.add(output, fasttext_model[lemmatizer.lemmatize(token)])\n",
    "        except KeyError:\n",
    "            output = numpy.add(output, numpy.zeros(300))\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total exception count: 14718\n",
      "No authors: 13841\n",
      "No titles: 0\n",
      "(101639, 18, 300)\n",
      "(101639, 1)\n",
      "done processing\n"
     ]
    }
   ],
   "source": [
    "global data\n",
    "global labels\n",
    "\n",
    "load_fasttext()\n",
    "json_data = get_data(\"coarse_discourse_dump_reddit.jsonlist\")\n",
    "data, labels = process_data(json_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 91475 samples, validate on 10164 samples\n",
      "Epoch 1/10\n",
      "91264/91475 [============================>.] - ETA: 0s - loss: 1.2205 - categorical_accuracy: 0.5938\n",
      "Epoch 00001: saving model to training_1/cp.ckpt\n",
      "91475/91475 [==============================] - 38s 419us/sample - loss: 1.2203 - categorical_accuracy: 0.5939 - val_loss: 1.1338 - val_categorical_accuracy: 0.6246\n",
      "Epoch 2/10\n",
      "91392/91475 [============================>.] - ETA: 0s - loss: 1.0496 - categorical_accuracy: 0.6449\n",
      "Epoch 00002: saving model to training_1/cp.ckpt\n",
      "91475/91475 [==============================] - 35s 386us/sample - loss: 1.0493 - categorical_accuracy: 0.6450 - val_loss: 1.0444 - val_categorical_accuracy: 0.6390\n",
      "Epoch 3/10\n",
      "91264/91475 [============================>.] - ETA: 0s - loss: 1.0089 - categorical_accuracy: 0.6539\n",
      "Epoch 00003: saving model to training_1/cp.ckpt\n",
      "91475/91475 [==============================] - 36s 389us/sample - loss: 1.0089 - categorical_accuracy: 0.6539 - val_loss: 1.0288 - val_categorical_accuracy: 0.6445\n",
      "Epoch 4/10\n",
      "91264/91475 [============================>.] - ETA: 0s - loss: 0.9847 - categorical_accuracy: 0.6591\n",
      "Epoch 00004: saving model to training_1/cp.ckpt\n",
      "91475/91475 [==============================] - 36s 389us/sample - loss: 0.9846 - categorical_accuracy: 0.6593 - val_loss: 1.0810 - val_categorical_accuracy: 0.6264\n",
      "Epoch 5/10\n",
      "91392/91475 [============================>.] - ETA: 0s - loss: 0.9648 - categorical_accuracy: 0.6629 ETA: 0s - loss: 0.9649 - categorical_accuracy: 0.66\n",
      "Epoch 00005: saving model to training_1/cp.ckpt\n",
      "91475/91475 [==============================] - 37s 401us/sample - loss: 0.9649 - categorical_accuracy: 0.6629 - val_loss: 1.0116 - val_categorical_accuracy: 0.6398\n",
      "Epoch 6/10\n",
      "91392/91475 [============================>.] - ETA: 0s - loss: 0.9480 - categorical_accuracy: 0.6679\n",
      "Epoch 00006: saving model to training_1/cp.ckpt\n",
      "91475/91475 [==============================] - 40s 443us/sample - loss: 0.9479 - categorical_accuracy: 0.6679 - val_loss: 1.0091 - val_categorical_accuracy: 0.6465\n",
      "Epoch 7/10\n",
      "91264/91475 [============================>.] - ETA: 0s - loss: 0.9330 - categorical_accuracy: 0.6709\n",
      "Epoch 00007: saving model to training_1/cp.ckpt\n",
      "91475/91475 [==============================] - 36s 391us/sample - loss: 0.9334 - categorical_accuracy: 0.6709 - val_loss: 1.0473 - val_categorical_accuracy: 0.6291\n",
      "Epoch 8/10\n",
      "91392/91475 [============================>.] - ETA: 0s - loss: 0.9226 - categorical_accuracy: 0.6749\n",
      "Epoch 00008: saving model to training_1/cp.ckpt\n",
      "91475/91475 [==============================] - 35s 384us/sample - loss: 0.9226 - categorical_accuracy: 0.6749 - val_loss: 1.0033 - val_categorical_accuracy: 0.6374\n",
      "Epoch 9/10\n",
      "91264/91475 [============================>.] - ETA: 0s - loss: 0.9108 - categorical_accuracy: 0.6772\n",
      "Epoch 00009: saving model to training_1/cp.ckpt\n",
      "91475/91475 [==============================] - 36s 397us/sample - loss: 0.9111 - categorical_accuracy: 0.6771 - val_loss: 1.0003 - val_categorical_accuracy: 0.6422\n",
      "Epoch 10/10\n",
      "91264/91475 [============================>.] - ETA: 0s - loss: 0.8995 - categorical_accuracy: 0.6817\n",
      "Epoch 00010: saving model to training_1/cp.ckpt\n",
      "91475/91475 [==============================] - 36s 393us/sample - loss: 0.8999 - categorical_accuracy: 0.6816 - val_loss: 1.0024 - val_categorical_accuracy: 0.6416\n"
     ]
    }
   ],
   "source": [
    "train(data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer='word', ngram_range=(2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "global body\n",
    "def process_body(load_data):\n",
    "    global vectorizer\n",
    "    global f\n",
    "    global body\n",
    "    global labels\n",
    "    count = 0\n",
    "    body_t = []\n",
    "    labels_t = []\n",
    "    for jline in load_data:\n",
    "        for post in jline['posts']:\n",
    "            try:\n",
    "                features = []\n",
    "                b = post['body']\n",
    "                p = f.getParentBody(jline, post['id'])\n",
    "                label = discourse.index(post['majority_type'])\n",
    "                features.append(b)\n",
    "                features.append(p)\n",
    "                body_t.append([b])\n",
    "                labels_t.append(label)\n",
    "            except Exception as e:\n",
    "                count += 1\n",
    "    print(count)\n",
    "    print(len(body))\n",
    "    body = numpy.array(body_t)\n",
    "    labels = numpy.array(labels_t)\n",
    "    return \"Done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14376\n",
      "101981\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Done'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101981, 1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer sequential_27 is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: [None, 1]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-8ecd7c097d01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mhub_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKerasLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhub_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-67-4600cc713301>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(body, labels, hub_layer)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mset_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhub_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mcp_callback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_weights_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mtrained_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcp_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Library/Python/3.6/lib/python/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    795\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/Library/Python/3.6/lib/python/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m           \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m           use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.6/lib/python/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[0;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    550\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         \u001b[0mcheck_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m         steps=steps_per_epoch)\n\u001b[0m\u001b[1;32m    553\u001b[0m     (x, y, sample_weights,\n\u001b[1;32m    554\u001b[0m      \u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.6/lib/python/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2311\u001b[0m     \u001b[0;31m# First, we build the model on the fly if necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2312\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2313\u001b[0;31m       \u001b[0mall_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_model_with_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2314\u001b[0m       \u001b[0mis_build_called\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2315\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.6/lib/python/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_build_model_with_inputs\u001b[0;34m(self, inputs, targets)\u001b[0m\n\u001b[1;32m   2537\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2538\u001b[0m       \u001b[0mcast_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2539\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2540\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mprocessed_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_dict_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.6/lib/python/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_set_inputs\u001b[0;34m(self, inputs, outputs, training)\u001b[0m\n\u001b[1;32m   2624\u001b[0m           \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'training'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2625\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2626\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2627\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2628\u001b[0m         \u001b[0;31m# This Model or a submodel is dynamic and hasn't overridden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.6/lib/python/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    735\u001b[0m         \u001b[0;31m# are casted, not before.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m         input_spec.assert_input_compatibility(self.input_spec, inputs,\n\u001b[0;32m--> 737\u001b[0;31m                                               self.name)\n\u001b[0m\u001b[1;32m    738\u001b[0m         if (any(isinstance(x, ragged_tensor.RaggedTensor) for x in input_list)\n\u001b[1;32m    739\u001b[0m             and self._supports_ragged_inputs is False):  # pylint: disable=g-bool-id-comparison\n",
      "\u001b[0;32m~/Library/Python/3.6/lib/python/site-packages/tensorflow_core/python/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    175\u001b[0m                          \u001b[0;34m'expected ndim='\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', found ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m                          \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'. Full shape received: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m                          str(x.shape.as_list()))\n\u001b[0m\u001b[1;32m    178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m       \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 of layer sequential_27 is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: [None, 1]"
     ]
    }
   ],
   "source": [
    "embedding = \"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\"\n",
    "global hub_layer\n",
    "hub_layer = hub.KerasLayer(embedding, input_shape=[], trainable=True, dtype=tf.string)\n",
    "print(body.shape)\n",
    "train(body, labels, hub_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_model(data):\n",
    "    global trained_model\n",
    "    input_shape = data[0].shape\n",
    "    # After trying a bunch of different methods, this one worked the best\n",
    "    t_model = tf.keras.Sequential([\n",
    "        \n",
    "        Bidirectional(LSTM(100, input_shape=(len(data), 2))),\n",
    "        #Bidirectional(LSTM(100)),\n",
    "        #Bidirectional(LSTM(64)),\n",
    "        #hub_layer,\n",
    "        Dropout(0.5),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.25),\n",
    "        Dense(32, activation='relu'),\n",
    "        \n",
    "        #tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    t_model.compile(optimizer='rmsprop',\n",
    "                    loss='categorical_crossentropy',\n",
    "                    metrics=['categorical_accuracy'])\n",
    "    trained_model = t_model\n",
    "\n",
    "def train(data, labels):\n",
    "    global trained_model\n",
    "    checkpoint_path = 'training_1/cp.ckpt'\n",
    "    set_model(data)\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, save_weights_only=True, verbose=1)\n",
    "    trained_model.fit(data, to_categorical(labels), validation_split=0.1, epochs=10, batch_size=128, callbacks=[cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = get_data(\"coarse_discourse_dump_reddit.jsonlist\")\n",
    "process_body(json_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
